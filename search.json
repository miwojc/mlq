[
  {
    "objectID": "20220902.html",
    "href": "20220902.html",
    "title": "Stock prices",
    "section": "",
    "text": "A fund manager wants George’s team to design a machine learning model that evaluates the price of stocks. The model should use publicly available information to predict the true share price of each company.\nThat’s all the fund manager needs. He will take it from there and look deeper into under-priced stocks (where the true price is higher than the current market price.)\nThe model will act as a pre-filter to sort out potential companies, and the fund manager will do a more detailed analysis before deciding whether to invest in the stock. The fund manager doesn’t care if there are occasional wrong predictions, even by a large margin, but he doesn’t want to have too many false positives because detailed research will cost a lot of time.\nGeorge is trying to decide which loss function the team should use for the model.\nWhich of the following loss functions do you think are suitable for this problem?\n\nMean Squared Error (MSE)\nMean Absolute Error (MAE)\nBinary Cross-entropy Loss\nHuber Loss\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2, 4\nWe don’t have too much information about the problem, but the client has clear priorities regarding the errors they are willing to tolerate. Significant outliers are not a big problem because the fund manager will spot them quickly and reject them. However, they expect excellent overall performance to avoid wasting time researching unpromising stocks.\nWith this in mind, the Mean Absolute Error (MAE) and the Huber loss seem like better options than the Mean Squared Error (MSE.) The MSE penalizes high errors of the model much more than the others. This means that the model will try to learn to avoid big mistakes in the prediction, which will likely come at the cost of worse overall performance. We don’t want this.\nOn the other hand, the MAE and the Huber loss are more robust to outliers, and they will not dominate the training process. Therefore, the second and fourth choices are the correct answers.\nThe cross-entropy loss doesn’t apply to regression problems, so the third choice is also incorrect.\nRecommended reading\n\nCheck “Regression Metrics for Machine Learning” for an overview of some of the most popular metrics used for regression problems.\n“RMSE vs MAE, which should I use?” is a great summary by Stephen Allwright about the properties of these two functions and how you should think about them. For more information about the Huber loss, take a look at “Huber Loss: Why Is It, Like How It Is?”."
  },
  {
    "objectID": "20220820.html",
    "href": "20220820.html",
    "title": "Abigail’s chart",
    "section": "",
    "text": "She was ready to share her model with the rest of her team. This was her first machine learning experience, and she was proud of how much progress she had made in the last few weeks.\nAbigail plotted her model’s training and validation loss over the number of iterations (epochs). The cart looked like this:\n\nWhich of the following is the correct conclusion from looking at Abigail’s plot?\n\nThe training loss decreases continuously, indicating that the model is doing well.\nThe validation loss decreases to a point and starts increasing again, indicating the model is overfitting.\nThe training and validation loss suddenly slowed down, indicating a problem with the learning process.\nThe validation loss doesn’t follow the training loss closely, indicating the model is underfitting.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nAnalyzing learning curves is one of the fundamental skills you should build in your career. There are different learning curves, but we will focus on Abigail’s chart.\nFirst, notice that the chart shows the loss—or error—as we increase the number of training iterations. A good mental model is to look at this the following way: “as we keep training, how much better the model gets?” Since we are displaying the loss, larger values are worse, so having both lines decrease is a good sign.\nWe have two lines in the chart: one representing the loss we get during training, the other representing the loss during the validation process. How these lines look concerning each other is essential. Most of the time, one of the lines alone wouldn’t give you a complete picture of the situation.\nLet’s start with the first choice that argues that a training loss that’s continually decreasing is good. Indeed this could be a good sign in isolation: the more we train our model, the fewer mistakes we make on the training dataset, but this is not enough to draw any conclusions.\nAlways be suspicious of an always-decreasing training loss; it’s a sign that your model might be memorizing the data. Usually, you want a model that learns up to a point, and then the loss stays flat. But how do you know when during the process that should happen? The relationship between the training loss and the validation loss shows how they start diverging at about 100 iterations. This point is the key.\nThink about it this way: Abigail’s model “continues learning” the training data but stops learning the validation data at about 100 iterations. The model is overfitting: it’s memorizing the training data, which is not helping with the validation data. Therefore, the first choice is incorrect, and the second is correct.\nThe third choice is incorrect as well. You’ll always see a similar slow down in the loss functions. The best case—not usual, but technically possible—is to reach 0, where there’s nowhere else to go.\nFinally, the fourth choice is also incorrect. First, the training and validation loss do not necessarily need to follow each other. Second, this model shows overfitting—memorizing the training data—and not underfitting—lack of model complexity to learn the data correctly. You can usually identify an underfitting model when its training loss is flat or doesn’t decrease much.\nIn summary, the second choice is the correct answer to this question.\nRecommended reading\n\n“How to use Learning Curves to Diagnose Machine Learning Model Performance” is the article that inspired this question and from where I borrowed Abigail’s chart.\nWikipedia’s introduction to Learning curves."
  },
  {
    "objectID": "20220828.html",
    "href": "20220828.html",
    "title": "The first exam",
    "section": "",
    "text": "Almost 50 students submitted a document explaining the neural network architecture they used to solve a problem and a complete analysis of their results.\nAyla opens the first paper and sees the following chart, showing the model’s training and validation loss over the number of iterations (epochs):\n\nWhich of the following is the correct feedback Ayla should give this student?\n\nThe training loss decreases continuously, indicating that the model is doing well.\nThe separation between the training and the validation curves indicates the training set is too complex for the model to learn.\nThe training and validation loss continue to decrease until the end of the training process, indicating the model is overfitting.\nThe training and validation loss continue to decrease until the end of the training process, indicating the model is underfitting.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n4\nAnalyzing learning curves is one of the fundamental skills you should build in your career. There are different learning curves, but we will focus on Ayla’s chart here.\nFirst, notice that the chart shows the loss—or error—as we increase the number of training iterations. A good mental model is to look at this the following way: “as we keep training, how much better the model gets?” Since we are displaying the loss, larger values are worse, so having both lines decrease is a good sign.\nWe have two lines in the chart: one representing the loss we get during training, the other representing the loss during the validation process. How these lines look concerning each other is essential. Most of the time, one of the lines alone wouldn’t give you a complete picture of the situation.\nLet’s start with the first choice that argues that a training loss that’s continually decreasing means that Ayla’s model is doing a good job. While this could be a good sign, it’s not a sufficient explanation to draw any conclusions about the quality of the model. For example, the model could be memorizing the dataset (overfitting), and while the training loss is decreasing, we will end up with a flawed model. Therefore, the first choice is incorrect.\nThe chart shows a gap between the training and the validation loss. Even when the student didn’t train the model for too long (about 50 epochs), the gap indicates the model can achieve a better loss in the validation dataset. This, however, doesn’t mean the training dataset is too complex for the model to learn. The chart shows how the training loss continually decreases until the end of the process, so the second choice is also incorrect.\nThe critical insight is that the training and validation losses decrease during the entire process. What would you say if you were to guess what would happen right after the 50th epoch? Looking at the chart, it seems clear that both the training and validation losses have the potential to keep decreasing, and therefore the student stopped the training process too early.\nWhat does this mean? Is the model overfitting or underfitting?\nThe model is not maximizing its potential. If we let it train for longer, the model will likely continue learning. This is a characteristic of underfitting models: they aren’t taking full advantage of their data. Therefore, the third choice is incorrect, and the fourth is the correct answer to this question.\nRecommended reading\n\n“How to use Learning Curves to Diagnose Machine Learning Model Performance” is the article that inspired this question and from where I borrowed Ayla’s chart.\nWikipedia’s introduction to Learning curves."
  },
  {
    "objectID": "20220827.html",
    "href": "20220827.html",
    "title": "Predictive maintenance",
    "section": "",
    "text": "After successfully testing the model in a pilot project, the company plans to start using it in one of its warehouses. Before they are ready, Miriam needs to summarize the performance of the pilot project.\nShe ran pictures of 100 different components through the model and found the following:\n\nThe model predicted that 7 components needed maintenance. After manually inspecting them, only 5 required work, but the other 2 were okay.\nThe model predicted that 93 components were working as expected but missed 2 that needed maintenance.\n\nWhich of the following is the correct summary of accuracy, precision, and recall for Miriam’s model?\n\nThe model’s accuracy is 93%, the precision is 29%, and the recall is 71%.\nThe model’s accuracy is 98%, the precision is 71%, and the recall is 29%.\nThe model’s accuracy is 90%, the precision is 98%, and the recall is 29%.\nThe model’s accuracy is 96%, the precision is 71%, and the recall is 71%.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n4\nThe easiest way to answer this question is to put the information we know in a confusion matrix. Whenever you build a classification model, the confusion matrix will help tremendously. Let’s see how.\nHere is the confusion matrix with Miriam’s results. Notice the small annotations next to each value:\n\n\nMiriam’s model found 5 true positives (TP). These are the components that the model predicted that needed maintenance that actually needed it.\nThere are 2 false positives (FP). These are the other 2 components the model thought erroneously needed maintenance.\nThere are 91 true negatives (TN). These are the components the model classified as okay and were indeed fine.\nThere are 2 false negatives (FN). These are the 2 components that the model missed as needing maintenance.\n\nWe can compute the three metrics we need using this information. Let’s start with accuracy:\naccuracy = (TP+TN)/(TP+TN+FP+FN)\naccuracy = (5+91)/(5+91+2+2)\naccuracy = 96/100\naccuracy = 0.96\nThe precision of the model is the fraction of components that truly need maintenance among the components that the model predicts need it. We can compute the precision this way:\nprecision = TP/(TP + FP)\nprecision = 5/(5 + 2)\nprecision = 5/7\nprecision = 0.71\nThe recall of the model is the fraction of components that truly need maintenance among all the existing components that need maintenance. We can compute the recall this way:\nrecall = TP/(TP + FN)\nrecall = 5/(5 + 2)\nrecall = 5/7\nrecall = 0.71\nAs you can see, the fourth choice is the correct answer to this question.\nRecommended reading\n\nHere is the Wikipedia page explaining what a confusion matrix is. Most of the information you need is here.\nCheck out “Precision and recall” for a better understanding of these two metrics.\n“Evaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall” is a great article that puts all of these concepts together."
  },
  {
    "objectID": "20220824.html",
    "href": "20220824.html",
    "title": "The political reporter",
    "section": "",
    "text": "In her short career as a political reporter, she has never dealt with technical data, so imagine her surprise when she realized the information her source sent her was not in plain English.\nThe hospital has been using a machine learning model to predict whether patients sick with COVID-19 will end up in the hospital. The report came with a bunch of numbers, and it measured the quality of the model using two weird terms: “precision” and “recall.”\nAria has to write her article, but first, she needs to figure out how to interpret the numbers.\nWhich of the following statements accurately represent what precision and recall are?\n\nPrecision: Among all the patients classified by the model as potential hospitalizations, the fraction that ended up in the hospital.\nPrecision: Among all the patients that ended up in the hospital, the fraction that the model correctly classified.\nRecall: Among all the patients classified by the model as potential hospitalizations, the fraction that ended up in the hospital.\nRecall: Among all the patients that ended up in the hospital, the fraction that the model correctly classified.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 4\nIn machine learning, especially when building classification models, precision and recall are two of the most critical metrics we need to understand.\nLet’s start with the definition from Wikipedia:\nPrecision is the fraction of relevant instances among the retrieved instances, while recall is the fraction of relevant instances that were retrieved.\nUsing this definition, we can break down what precision is by looking at two components:\n\nRelevant instances: These are all patients the model predicted as positive and actually ended up in the hospital. We call these “true positive” patients.\nRetrieved instances: These are all the patients the model predicted would end up in the hospital.\n\nThe precision is the number of relevant instances divided by retrieved instances. We want to express how “precise” the model was: out of every patient the model predicted would end up in the hospital, how many were actually hospitalized. Looking at the available choices, the first is the correct definition of precision.\nLet’s do the same with recall. We need two components:\n\nRelevant instances: These are all patients the model predicted as positive and actually ended up in the hospital. We call these “true positive” patients.\nPositive instances: The number of patients that ended up in the hospital.\n\nThe recall is the number of relevant instances divided by the number of positive instances. In other words, we want to express the capacity of the model to find positive patients: out of every patient that ended up in the hospital, how many did the model retrieve. Looking at the choices, the fourth is the correct definition of recall.\nIn summary, the first and fourth choices are the correct answer to this question.\nRecommended reading\n\nCheck out “Precision and recall” for a better understanding of these two metrics.\n“Precision vs Recall” is a great article covering the tradeoffs of precision and recall using a real-life example."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Questions by bnomial",
    "section": "",
    "text": "This website is built with markdown and Quarto"
  },
  {
    "objectID": "20220817.html",
    "href": "20220817.html",
    "title": "A classification model",
    "section": "",
    "text": "A critical step she wants to take is introducing machine learning into her work. She started learning some of the fundamentals and is now ready to apply what she’s learned.\nShe researched one of her company’s problems and learned she needed to build a supervised learning classification model. She had enough labeled data, so it seemed like a good fit.\nBased on this, which of the following better describes what Natalie needs to accomplish?\n\nShe needs to train a model that returns a numerical prediction for each sample of data.\nShe needs to train a model that clusters the data into different groups based on their characteristics.\nShe needs to train a model to predict the class of every sample of data out of a predefined list of classes.\nShe needs to train a model that returns the optimal policy that maximizes the potential outcomes of her problem.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n3\nNatalie’s problem requires her to predict the class of every sample of data out of a predefined list of classes. That’s the goal of machine learning classification models.\nThe first choice refers to a regression model. Here we want the model to output a single, continuous value. For example, imagine we want to return the predicted price of a house or the predicted highest temperature for the weekend.\nThe second choice refers to a clustering model. These unsupervised learning techniques are helpful when we don’t have labels for our data and want the algorithm to group every sample into dynamically generated groups.\nThe fourth choice is a loose description of a reinforcement learning approach, where we want an agent to learn the optimal policy that maximizes a reward function.\nRecommended reading\n\n“4 Types of Classification Tasks in Machine Learning” is an excellent introduction to classification models in machine learning.\nFor an introduction to classification models, check “Classification in Machine Learning: What it is and Classification Models”"
  },
  {
    "objectID": "20220823.html",
    "href": "20220823.html",
    "title": "Bloody snake",
    "section": "",
    "text": "Don’t ask how you got here. You don’t have time for these questions. You need to focus on staying alive.\nA massive snake with three heads and bloody eyes comes charging after you. It’s not looking good!\nYou have enough breath to read a clue written on the walls at the last second: “bootstrap it, and you’ll live,” it says.\nWhere would you hide? Only one of these places will save your life:\n\nBehind a tree\nIn the forest\nIn a cave\nIn a hole\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nI hope this was a fun question.\nAs you would expect, the answer is in the clue written on the walls. “Bootstrap it, and you’ll live” doesn’t say much until you see the four hiding spots: A tree, a forest, a cave, and a hole.\nBootstrap aggregating, also called “bagging,” is a popular machine learning ensembling technique. We usually use bagging with decision trees, and one of the hiding spots is a tree, so that could be the answer! In reality, there’s an even better answer.\nRandom Forest is an algorithm that consists of many individual decision trees. It uses bootstrap aggregating to combine these trees to reach a solution much better than the one provided by any of the individual trees.\n“Bootstrapping” is more closely related to Random Forest than Decision Trees, so you’ll stay alive if you hide in the forest.\nRecommended reading\n\n“Bagging and Random Forest Ensemble Algorithms for Machine Learning” is a great introduction to bagging and Random Forest.\nCheck out “Understanding Random Forest” to understand how it works and why it’s effective."
  },
  {
    "objectID": "20220831.html",
    "href": "20220831.html",
    "title": "Vintage shoes",
    "section": "",
    "text": "Business is going great, and one of her main issues is determining the price of new inventory as it comes in. Sam would like to create a machine learning model that automatically selects the best price using the historical performance.\nSam was a data scientist in a previous life, so she was surprised when she found some documentation recommending a new loss function: The Root Mean Squared Log Error (RMSLE.) This function seemed better than Root Mean Squared Error (RMSE) for her use case.\nWhich of the following are some of the differences between RMSLE and RMSE?\n\nRMSLE penalizes coming under the actual value much more than coming above the actual value. On the other hand, RMSE penalizes both cases in the same way.\nRMSLE is not sensitive to outliers as RMSE is. Whenever we have outliers, the result of RMSE can explode while RMSLE will scale down the outliers.\nRMSLE measures the error by only focusing on the predicted value. On the other hand, RMSE uses the difference between the predicted and the actual value to compute the error.\nRMSLE focuses on the relative error between two values. When using RMSLE, the scale of the error is not significant. On the other hand, the result of RMSE increases in magnitude if the error scale increases.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 2, 4\nCompared to RMSE, RMSLE is a relatively new metric. Here is the formula:\n\\[\n\\sqrt{ \\frac{1}{n} \\sum_{i=1}^n (\\log(x_i + 1) - \\log(y_i + 1))^2 }\n\\]\nIf you compare this formula with the RMSE formula, you’ll notice they are almost the same, with the difference of RMSLE computing the log of both the predicted and actual values. This small difference comes with some essential properties.\nFirst, RMSLE penalizes underestimates much more than overestimates. To understand the reason, take a look at the following charts that I took from “What’s the Difference Between RMSE and RMSLE?”:\n\nNotice how RMSLE penalizes negative values much more heavily than positive ones, while RMSE equally penalizes both cases. Therefore, the first choice is correct.\nThe second choice is also correct. The log that RMSLE uses squashes outliers, while RMSE amplifies their effect. If you want your model to remain unaffected by outliers, RMSLE is a good candidate.\nThe third choice is incorrect because both RMSE and RMSLE use the predicted and actual values. Predicted values alone don’t tell us anything; comparing them with the actual value is critical.\nFinally, the fourth choice is also correct. For example, imagine a case where the model predicts 30 when the actual value was 40 and another case where the prediction was 300 when the actual value was 400. The RMSE of the second case is ten times more than the RMSE of the first case. However, the RMSLE will score both cases the same. The RMSLE measures the relative error, while the RMSE measures the absolute error.\nIn summary, the first, second, and fourth choices are correct.\nRecommended reading\n\n“What’s the Difference Between RMSE and RMSLE?” covers really well every one of the differences between these two functions.\nAnother article that covers the differences between some of the most popular functions is “Evaluation Metrics for Regression models”"
  },
  {
    "objectID": "20220818.html",
    "href": "20220818.html",
    "title": "Maximum depth",
    "section": "",
    "text": "But unfortunately, that’s not the only thing the classroom had to remember for their test. There were many more hyperparameters with funny names and responsibilities, and they barely had time to prepare for the exam.\nAs soon as Isabella looked at the first section of the test, her heart dropped. It was all about decision trees, and she spent most of her time focusing on neural networks.\nThe first question didn’t seem too tricky, however.\nWhich of the following is the correct definition of the maximum depth of a decision tree?\n\nThe maximum depth of a decision tree is the length of the longest path from the tree’s root to a leaf.\nThe maximum depth of a decision tree is the length of the shortest path from the tree’s root to a leaf.\nThe maximum depth of a decision tree is the length of the longest path from the tree’s root to an interior node.\nThe maximum depth of a decision tree is the length of the shortest path from the tree’s root to an interior node.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1\nIn a few words, a decision tree represents particular observations about a sample in its branches and the conclusions about that item’s target value as the tree’s leaves. If you draw the tree, its maximum depth is a way to explain how tall the tree is.\nIt makes sense that the maximum depth is the length of the longest path from the root to the bottom of the tree, but what exactly is that bottom? Is it a leaf or an internal node?\nYour decision tree would not be complete if you only considered internal nodes. Remember, leaves are where the conclusions of the target value live. In other words, a leaf is where the final answer lives. You can’t talk about the maximum depth of a tree without considering the leaves, so that’s the correct answer to this question.\nOne more important thing to notice is that everything else being equal, a more straightforward decision tree will always generalize better than a more complex one. The maximum depth of the tree directly affects its complexity.\nIn summary, the first choice is the correct answer to this question.\nRecommended reading\n\n“Decision tree learning” is the Wikipedia page where you can read about decision trees.\n“How to tune a Decision Tree?” is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth hyperparameter."
  },
  {
    "objectID": "20220826.html",
    "href": "20220826.html",
    "title": "The weird puzzle room",
    "section": "",
    "text": "But this room wasn’t like every other room.\nThey found the final puzzle behind a painting hanging from the wall. Four numbered little doors. The key hid behind one of them and a losing score behind the other three.\nAbove the doors, they found the question: “What’s the optimal depth of a decision tree?”\nWhat door will let Vivian’s family out of the room?\n\nThe optimal value for a decision tree’s depth is setting it to the number of training samples minus one.\nThe optimal value for a decision tree’s depth is setting it to the logarithm of the number of training samples.\nThe optimal value for a decision tree’s depth is setting it as low as we possibly can.\nThe optimal value for a decision tree’s depth is setting it as high as we possibly can.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n3\nWe need to revisit a few concepts and establish constraints to answer this question. How do we know a decision tree has an optimal depth?\nFirst, the depth of a decision tree is the length of the longest path from a root to a leaf. There’s a tradeoff to keep in mind related to this value. Deeper trees will lead to more complex models prone to overfitting. Shallower trees will have the opposite problem: less complex models prone to underfitting. Finding the appropriate depth is critical to getting good results.\nTo determine the optimal depth, we then need to establish how we will measure the performance of the decision tree. Since the goal is to build a model that produces good predictions, I’ll assume that our goal is to find the optimal depth to get the best predictions possible on a test dataset.\nLet’s start with the first choice that argues that we should set the depth at the number of training samples minus one. This is the maximum theoretical depth of a decision tree, but it’s not the optimal value. If we make the tree this deep, we will undoubtedly overfit the training data and perform poorly on any unseen data.\nThe second choice is also incorrect. Although this option is probably better than setting the depth to the number of samples, it will still lead to overfitting. For example, for a dataset with 1024 training samples, we will need a decision tree with 10 levels and a tree of depth 10, which requires a total of 2^10 = 1024 nodes. This choice will lead to a tree with as many nodes as samples, which will cause the tree to overfit.\nWe can analyze the third and fourth choices together. Are we better off setting the depth of a decision tree as low or as high as possible? Let’s pick two hypothetical decision trees, each producing the same results, but one deeper than the other. Which of these two models would you use?\nLess complex classifiers will usually generalize better than more complex ones. We should remove any non-critical or redundant sections of a decision tree, which generally leads to a much better model. Decision tree pruning is one technique used to accomplish this.\nIf you have two decision trees with the same predictive power in your test dataset, always pick the simpler one. Therefore, we should always set the depth as low as possible without affecting the model’s predictive capabilities.\nIn summary, the third choice is the correct answer to this question.\nRecommended reading\n\n“How to tune a Decision Tree?” is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth hyperparameter.\n“Decision tree pruning” is a Wikipedia article covering pruning and its effects.\nA great article to understand the relationship between the bias-variance tradeoff, overfitting, and under-fitting is “How To Find Decision Tree Depth via Cross-Validation”."
  },
  {
    "objectID": "20220822.html",
    "href": "20220822.html",
    "title": "Three elevators",
    "section": "",
    "text": "The views were amazing, but the building was busy, and getting an elevator during peak hours was always a chore.\nIris’s floor had three elevators unequally spaced along a wall. On a dull afternoon, while waiting for their ride to come, Iris asked her friends where they would stand and wait for the elevator. She wanted to minimize the distance they had to walk when one of the elevators arrived.\nWhich of the following will minimize the distance they have to walk? Keep in mind the elevators are not spaced equally along the wall.\n\nThey should always stand in any of the elevators at both extremes of the wall.\nThey should always stand in front of the middle elevator, regardless of its location along the wall.\nThey should always stand at the mid-point between the two elevators located at the extreme.\nThey should only stand in front of the middle elevator if the distance to the other two elevators is the same. If not, they can stand in any of the extreme elevators.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nIf Iris stands in front of the middle elevator, she will minimize the distance she has to walk. This holds for any distribution of the elevators along the wall—they don’t need to be equally spaced.\nLet’s break this down into the three possible situations with an example where we have elevators A, B, and C located along a wall (paraphrasing the answer on this Stack Exchange page):\n\nIf Iris waits in front of elevator A and elevator B arrives, she will need to walk from A to B. If elevator C arrives, she must walk from A to C, passing B on her way. Here is the pattern: A→B or A→B→C.\nIf Iris waits in front of elevator C and elevator A arrives, she will need to walk from C to A, passing B on her way. If elevator B comes, she must walk from C to B. Here is the pattern: C→B→A, or C→B.\nIf Iris waits in front of elevator B and elevator A arrives, she will need to walk from B to A. If elevator C arrives, she must walk from B to C. She will never have to pass any other elevator. Here is the pattern: B→A or B→C.\n\nDo you see how standing in front of any of the elevators at both extremes of the wall will make you walk the same distance in multiple cases? A→B in the first case, and C→B in the second one.\n“Visualizing the Median as the Minimum-Deviation Location” explains this as follows:\n\nFirst, consider waiting at a median location. Then, consider “moving” to a waiting position away from this median point. If one does so, one moves away from more elevators than one moves towards, thereby increasing the sum of the distances to the elevators.\n\nA more exciting insight: we can minimize the distance by standing at the median point between elevators for any number of elevators, not only three. Notice that this median point is not necessarily in front of an elevator.\nNow that we know the correct answer is the second choice imagine if we wanted to minimize the total distance they have to walk from the door they enter the lobby where the elevators are located. It’s no longer about the distance between the elevators, but we also need to consider the distance from the door to the elevator that arrives. The answer may surprise you.\nRecommended reading\n\nTake a look at “Visualizing the Median as the Minimum-Deviation Location” for an in-depth analysis of this problem.\n“Why does minimizing the MAE lead to forecasting the median and not the mean?” is the question that led me down this rabbit hole."
  },
  {
    "objectID": "20220830.html",
    "href": "20220830.html",
    "title": "Softmax mockery",
    "section": "",
    "text": "Alan was cooky. He didn’t know much but liked to show off with every bit of trivia that crossed his path. More than anything, he enjoyed pissing off Loretta at every opportunity he had.\nAnd here is Alan, bugging Loretta about the softmax function and ready to mock her if she fails.\nWhich of the following are correct statements about the softmax function?\n\nThe softmax function is a soft or smooth approximation to the max function.\nThe softmax function is a soft or smooth approximation to the argmax function.\nThe softmax function converts a vector of n values into a probability distribution of n possible outcomes.\nSoftmax is a differentiable function.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2, 3, 4\nThe name “softmax” is a misnomer. The softmax function is a smooth—soft—approximation of the argmax function, not the max function. To avoid this confusion, some of the literature uses “softargmax” instead, but the machine learning world ran with “softmax” and never looked back.\nThe softmax function turns a vector of n values into another vector of n probabilities that sum to 1. These probabilities are proportional to the relative scale of each value in the vector. For example, when we use softmax as the activation function of a neural network’s output layer, we get a normalized probability distribution that we can use to interpret the result of multi-class classification models.\nFinally, contrary to the argmax function, the softmax function is differentiable.\nRecommended reading\n\nCheck “The Softmax function and its derivative” for a complete explanation of softmax.\n“What is the Softmax Function?” is another great explanation of how the function works."
  },
  {
    "objectID": "20220819.html",
    "href": "20220819.html",
    "title": "Hiring a team",
    "section": "",
    "text": "Jordan was one of the first. She came right off her Ph.D. to lead the creation of an automated data pipeline for the system. There was only one small problem: she didn’t have industry experience, and her research work was very different.\nJordan needs to build a team, but it’s hard to identify the appropriate candidates without fully understanding what goes into a data pipeline.\nWhich of the following are some of the data processing steps that go into a data pipeline?\n\nData analysis is one of the steps of a data pipeline.\nData ingestion is one of the steps of a data pipeline.\nData preparation is one of the steps of a data pipeline.\nData validation is one of the steps of a data pipeline.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2, 3, 4\nA critical part of any production-ready machine learning system is the data pipeline. Its goal is to collect and prepare the data we can use later to train, validate and use the model.\nA data pipeline is a fully automated process. There shouldn’t be any steps that require human intervention.\nAnalyzing the data is not part of a data pipeline. During the data analysis phase, the team will define the data required by the system. This is a manual step that happens before the data pipeline is ready. During the data analysis phase, the team will make the necessary decisions to build the data pipeline. Therefore, the first option is incorrect.\nData ingestion is one of the steps of a data pipeline. During this step, we ingest the data from the different data sources and transfer it to where we will store it and later process it. For example, the company may need data currently stored in spreadsheets, databases, and a few real-time sensors. The data ingestion step will get the necessary data from each place and keep them in the final location for the rest of the process.\nData preparation is another essential step of a data pipeline, where we format and prepare the data before using it. For example, the company might want to standardize the format of dates collected from different sources or perform specific transformations on numerical data.\nFinally, data validation is another step of a data pipeline. We can validate that the necessary data we are ingesting is present and follow the appropriate schema during this step. For example, we might need to collect a minimum number of samples daily and raise an exception during the data validation step if we don’t meet that condition.\nRecommended reading\n\nThe “Machine Learning Data Lifecycle in Production” course in Coursera, part of the Machine Learning Engineering for Production (MLOps) Specialization.\n“What is a Data Pipeline” is a short article explaining the high-level idea of data pipelines."
  },
  {
    "objectID": "20220901.html",
    "href": "20220901.html",
    "title": "Bounding boxes",
    "section": "",
    "text": "She wants to measure the performance of her object detection method, but there is a problem: The bounding boxes predicted by her model never match precisely the bounding boxes in the ground truth data.\nMagdalena needs a metric that tells her how well the two bounding boxes overlap. Based on this metric, she can define a threshold and decide if the predicted bounding box is correct or not.\nWhich metric can Magdalena use?\n\nThe percentage of the ground truth bounding box that’s covered by the predicted bounding box.\nThe percentage of the predicted bounding box that’s covered by the ground truth bounding box.\nThe intersection area of the predicted and ground truth bounding boxes divided by their union.\nThe intersection area of the predicted and ground truth bounding boxes multiplied by their union.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n3\nAfter a quick look at the choices, the first two options sound like plausible solutions. However, they have fundamental flaws that may allow a bad model to score high on these metrics.\nLet’s take the percentage of the ground-truth bounding box covered by the prediction. If our model learns to predict huge bounding boxes covering almost the whole image, we will always get a score close to 100%.\nIf we take the percentage of the predicted bounding box covered by the ground truth, we have the opposite problem: a small bounding box somewhere in the bounds of the ground truth will give us a 100% score. Therefore the first two choices are incorrect.\nAn excellent way to deal with these problematic cases is to take the so-called intersection over union measure (IoU for short, also called the Jaccard index). We compute the intersection area of the bounding boxes and divide it by their union. The union will increase if the predicted bounding box is too big, driving the score down. The small intersection area will lower the score if the bounding box is too small.\nFinally, the last choice is also incorrect. Multiplying the intersection by the union will not give us any relevant or helpful results.\nRecommended reading\n\nCheck “Intersection over Union (IoU) for object detection” for an entire explanation of this metric."
  },
  {
    "objectID": "20220825.html",
    "href": "20220825.html",
    "title": "Hailey’s labeling approach",
    "section": "",
    "text": "Despite having access to a massive dataset of x-ray pictures, she didn’t have too many labels. The company couldn’t afford to have doctors spending their time labeling samples, so she was in a catch-22 situation: to save the doctor’s time, she wanted to build a supervised learning model, but she couldn’t do that without having the doctors spend more time labeling data.\nHailey found a way to solve the problem: she built a model using the labels she had. Then started processing the unlabeled samples and setting aside those for which her model wasn’t confident.\nAfter each round, Hailey asked doctors only to spend time labeling those low-confidence samples, after which she built a new model and repeated the process.\nThis approach saved the company a massive amount of money.\nHow would you classify Hailey’s approach?\n\nHailey’s approach is a Weak Supervision technique.\nHailey’s approach is a Reinforcement Learning technique.\nHailey’s approach is an Unsupervised Learning technique.\nHailey’s approach is a Semi-supervised Learning technique.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n4\nHailey encountered a widespread problem we often face when we want to build machine learning models for companies. Sometimes, we have access to plenty of data, but labeling the dataset is time-consuming or hard to collect without a prohibitive cost.\nHailey did something clever: she only asked doctors to spend time labeling images that had the most value to her model. She built a good model with a fraction of the labels that otherwise would have been necessary.\nWe call this technique “Active Learning”, a semi-supervised learning technique. Hailey followed a pool-based sampling approach, where she queried the pool of unlabeled samples and used the confidence of her model to decide which instances to label next.\nRecommended reading\n\n“Active Learning” is a short introduction to active learning and how the process works.\nIf you are serious about Active Learning, “Active Learning Literature Survey” is the publication you want to read."
  },
  {
    "objectID": "20220829.html",
    "href": "20220829.html",
    "title": "Distance measures",
    "section": "",
    "text": "Every distance measure doesn’t work for every problem. Instead, we must select the appropriate function depending on the nature of the data.\nHere is a list of four distance measures and a quick summary of how they work. Select every one of them that’s correct.\n\nThe Euclidean distance between two vectors is the square root of the sum of the squared differences between them.\nThe Manhattan distance between two vectors is the sum of their absolute differences.\nThe Hamming distance is a generalization of the Euclidean and Manhattan distances that we can tune depending on which distance measure we need.\nThe Minkowski distance computes the distance between two binary vectors.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 2\nWhile the Euclidean and Manhattan distances summary is correct, the Hamming and Minkowski summaries aren’t.\nThe Hamming distance computes the distance between two binary vectors. For example, you can calculate the distance between objects using a one-hot encoded feature.\nThe Minkowski distance is a generalization of the Euclidean and Manhattan distances. Both of these work with real-value vectors, but the Euclidean distance is the shortest path between objects, while the Manhattan distance is the rectilinear distance between them. Using the Minkowski distance, we can control which approach to use depending on the data.\nRecommended reading\n\nCheck “4 Distance Measures for Machine Learning” for a complete explanation of these four distance measures.\n“Five Common Distance Measures in Data Science With Formulas and Examples” is a deeper dive into these distance measures."
  },
  {
    "objectID": "20220821.html",
    "href": "20220821.html",
    "title": "Low-bias models",
    "section": "",
    "text": "High bias models are simpler to interpret and usually run fast, but they don’t scale well to complex problems.\nJuliette has already tried a few high-bias algorithms on her dataset and pushed them as far as possible. She is now ready to try something different.\nWhich of the following algorithms are low-bias models that Juliette can try on her problem?\n\nLinear Regression\nDecision Trees\nLogistic Regression\nk-Nearest Neighbors (KNN)\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2, 4\nEvery machine learning algorithm deals with three types of errors: bias, variance, and irreducible error. We need to focus specifically on the bias error to answer this question.\nHere is what Jason Brownlee has to say about bias: “Bias are the simplifying assumptions made by a model to make the target function easier to learn.”\nIn other words, “bias” refers to the assumptions the model makes to simplify the process of finding answers. The fewer assumptions it makes, the less biased the model is.\nNonlinear models are usually low-bias. They don’t make too many assumptions about the target function, making them an excellent option for tackling complex problems. Decision Trees and k-Nearest Neighbors are examples of low-bias models.\nOn the other hand, linear models are usually high-bias. They are easier to understand but make too many assumptions about the target function, preventing them from performing well on complex problems. Linear and logistic regression are two examples of high-bias models.\nRecommended reading\n\nHere is Jason Brownlee’s article I mentioned before: “Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning”.\nThe Wikipedia page on bias and variance is also a good resource: “Bias–variance tradeoff”."
  }
]