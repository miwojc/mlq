[
  {
    "objectID": "20220820.html",
    "href": "20220820.html",
    "title": "Abigail’s chart",
    "section": "",
    "text": "She was ready to share her model with the rest of her team. This was her first machine learning experience, and she was proud of how much progress she had made in the last few weeks.\nAbigail plotted her model’s training and validation loss over the number of iterations (epochs). The cart looked like this:\n\nWhich of the following is the correct conclusion from looking at Abigail’s plot?\n\nThe training loss decreases continuously, indicating that the model is doing well.\nThe validation loss decreases to a point and starts increasing again, indicating the model is overfitting.\nThe training and validation loss suddenly slowed down, indicating a problem with the learning process.\nThe validation loss doesn’t follow the training loss closely, indicating the model is underfitting.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nAnalyzing learning curves is one of the fundamental skills you should build in your career. There are different learning curves, but we will focus on Abigail’s chart.\nFirst, notice that the chart shows the loss—or error—as we increase the number of training iterations. A good mental model is to look at this the following way: “as we keep training, how much better the model gets?” Since we are displaying the loss, larger values are worse, so having both lines decrease is a good sign.\nWe have two lines in the chart: one representing the loss we get during training, the other representing the loss during the validation process. How these lines look concerning each other is essential. Most of the time, one of the lines alone wouldn’t give you a complete picture of the situation.\nLet’s start with the first choice that argues that a training loss that’s continually decreasing is good. Indeed this could be a good sign in isolation: the more we train our model, the fewer mistakes we make on the training dataset, but this is not enough to draw any conclusions.\nAlways be suspicious of an always-decreasing training loss; it’s a sign that your model might be memorizing the data. Usually, you want a model that learns up to a point, and then the loss stays flat. But how do you know when during the process that should happen? The relationship between the training loss and the validation loss shows how they start diverging at about 100 iterations. This point is the key.\nThink about it this way: Abigail’s model “continues learning” the training data but stops learning the validation data at about 100 iterations. The model is overfitting: it’s memorizing the training data, which is not helping with the validation data. Therefore, the first choice is incorrect, and the second is correct.\nThe third choice is incorrect as well. You’ll always see a similar slow down in the loss functions. The best case—not usual, but technically possible—is to reach 0, where there’s nowhere else to go.\nFinally, the fourth choice is also incorrect. First, the training and validation loss do not necessarily need to follow each other. Second, this model shows overfitting—memorizing the training data—and not underfitting—lack of model complexity to learn the data correctly. You can usually identify an underfitting model when its training loss is flat or doesn’t decrease much.\nIn summary, the second choice is the correct answer to this question.\nRecommended reading\n\n“How to use Learning Curves to Diagnose Machine Learning Model Performance” is the article that inspired this question and from where I borrowed Abigail’s chart.\nWikipedia’s introduction to Learning curves."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Questions by bnomial",
    "section": "",
    "text": "This website is built with markdown and Quarto"
  },
  {
    "objectID": "20220817.html",
    "href": "20220817.html",
    "title": "A classification model",
    "section": "",
    "text": "A critical step she wants to take is introducing machine learning into her work. She started learning some of the fundamentals and is now ready to apply what she’s learned.\nShe researched one of her company’s problems and learned she needed to build a supervised learning classification model. She had enough labeled data, so it seemed like a good fit.\nBased on this, which of the following better describes what Natalie needs to accomplish?\n\nShe needs to train a model that returns a numerical prediction for each sample of data.\nShe needs to train a model that clusters the data into different groups based on their characteristics.\nShe needs to train a model to predict the class of every sample of data out of a predefined list of classes.\nShe needs to train a model that returns the optimal policy that maximizes the potential outcomes of her problem.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n3\nNatalie’s problem requires her to predict the class of every sample of data out of a predefined list of classes. That’s the goal of machine learning classification models.\nThe first choice refers to a regression model. Here we want the model to output a single, continuous value. For example, imagine we want to return the predicted price of a house or the predicted highest temperature for the weekend.\nThe second choice refers to a clustering model. These unsupervised learning techniques are helpful when we don’t have labels for our data and want the algorithm to group every sample into dynamically generated groups.\nThe fourth choice is a loose description of a reinforcement learning approach, where we want an agent to learn the optimal policy that maximizes a reward function.\nRecommended reading\n\n“4 Types of Classification Tasks in Machine Learning” is an excellent introduction to classification models in machine learning.\nFor an introduction to classification models, check “Classification in Machine Learning: What it is and Classification Models”"
  },
  {
    "objectID": "20220818.html",
    "href": "20220818.html",
    "title": "Maximum depth",
    "section": "",
    "text": "But unfortunately, that’s not the only thing the classroom had to remember for their test. There were many more hyperparameters with funny names and responsibilities, and they barely had time to prepare for the exam.\nAs soon as Isabella looked at the first section of the test, her heart dropped. It was all about decision trees, and she spent most of her time focusing on neural networks.\nThe first question didn’t seem too tricky, however.\nWhich of the following is the correct definition of the maximum depth of a decision tree?\n\nThe maximum depth of a decision tree is the length of the longest path from the tree’s root to a leaf.\nThe maximum depth of a decision tree is the length of the shortest path from the tree’s root to a leaf.\nThe maximum depth of a decision tree is the length of the longest path from the tree’s root to an interior node.\nThe maximum depth of a decision tree is the length of the shortest path from the tree’s root to an interior node.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1\nIn a few words, a decision tree represents particular observations about a sample in its branches and the conclusions about that item’s target value as the tree’s leaves. If you draw the tree, its maximum depth is a way to explain how tall the tree is.\nIt makes sense that the maximum depth is the length of the longest path from the root to the bottom of the tree, but what exactly is that bottom? Is it a leaf or an internal node?\nYour decision tree would not be complete if you only considered internal nodes. Remember, leaves are where the conclusions of the target value live. In other words, a leaf is where the final answer lives. You can’t talk about the maximum depth of a tree without considering the leaves, so that’s the correct answer to this question.\nOne more important thing to notice is that everything else being equal, a more straightforward decision tree will always generalize better than a more complex one. The maximum depth of the tree directly affects its complexity.\nIn summary, the first choice is the correct answer to this question.\nRecommended reading\n\n“Decision tree learning” is the Wikipedia page where you can read about decision trees.\n“How to tune a Decision Tree?” is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth hyperparameter."
  },
  {
    "objectID": "20220819.html",
    "href": "20220819.html",
    "title": "Hiring a team",
    "section": "",
    "text": "Jordan was one of the first. She came right off her Ph.D. to lead the creation of an automated data pipeline for the system. There was only one small problem: she didn’t have industry experience, and her research work was very different.\nJordan needs to build a team, but it’s hard to identify the appropriate candidates without fully understanding what goes into a data pipeline.\nWhich of the following are some of the data processing steps that go into a data pipeline?\n\nData analysis is one of the steps of a data pipeline.\nData ingestion is one of the steps of a data pipeline.\nData preparation is one of the steps of a data pipeline.\nData validation is one of the steps of a data pipeline.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2, 3, 4\nA critical part of any production-ready machine learning system is the data pipeline. Its goal is to collect and prepare the data we can use later to train, validate and use the model.\nA data pipeline is a fully automated process. There shouldn’t be any steps that require human intervention.\nAnalyzing the data is not part of a data pipeline. During the data analysis phase, the team will define the data required by the system. This is a manual step that happens before the data pipeline is ready. During the data analysis phase, the team will make the necessary decisions to build the data pipeline. Therefore, the first option is incorrect.\nData ingestion is one of the steps of a data pipeline. During this step, we ingest the data from the different data sources and transfer it to where we will store it and later process it. For example, the company may need data currently stored in spreadsheets, databases, and a few real-time sensors. The data ingestion step will get the necessary data from each place and keep them in the final location for the rest of the process.\nData preparation is another essential step of a data pipeline, where we format and prepare the data before using it. For example, the company might want to standardize the format of dates collected from different sources or perform specific transformations on numerical data.\nFinally, data validation is another step of a data pipeline. We can validate that the necessary data we are ingesting is present and follow the appropriate schema during this step. For example, we might need to collect a minimum number of samples daily and raise an exception during the data validation step if we don’t meet that condition.\nRecommended reading\n\nThe “Machine Learning Data Lifecycle in Production” course in Coursera, part of the Machine Learning Engineering for Production (MLOps) Specialization.\n“What is a Data Pipeline” is a short article explaining the high-level idea of data pipelines."
  },
  {
    "objectID": "20220821.html",
    "href": "20220821.html",
    "title": "Low-bias models",
    "section": "",
    "text": "High bias models are simpler to interpret and usually run fast, but they don’t scale well to complex problems.\nJuliette has already tried a few high-bias algorithms on her dataset and pushed them as far as possible. She is now ready to try something different.\nWhich of the following algorithms are low-bias models that Juliette can try on her problem?\n\nLinear Regression\nDecision Trees\nLogistic Regression\nk-Nearest Neighbors (KNN)\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2, 4\nEvery machine learning algorithm deals with three types of errors: bias, variance, and irreducible error. We need to focus specifically on the bias error to answer this question.\nHere is what Jason Brownlee has to say about bias: “Bias are the simplifying assumptions made by a model to make the target function easier to learn.”\nIn other words, “bias” refers to the assumptions the model makes to simplify the process of finding answers. The fewer assumptions it makes, the less biased the model is.\nNonlinear models are usually low-bias. They don’t make too many assumptions about the target function, making them an excellent option for tackling complex problems. Decision Trees and k-Nearest Neighbors are examples of low-bias models.\nOn the other hand, linear models are usually high-bias. They are easier to understand but make too many assumptions about the target function, preventing them from performing well on complex problems. Linear and logistic regression are two examples of high-bias models.\nRecommended reading\n\nHere is Jason Brownlee’s article I mentioned before: “Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning”.\nThe Wikipedia page on bias and variance is also a good resource: “Bias–variance tradeoff”."
  }
]