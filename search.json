[
  {
    "objectID": "20221006.html",
    "href": "20221006.html",
    "title": "A clever loss function",
    "section": "",
    "text": "During training, Adalynn runs three samples simultaneously through her model:\n\nAn anchor image\nA matching picture representing the same entity as the anchor\nA non-matching picture representing a different entity as the anchor\n\nThe loss function she uses takes these three inputs and minimizes the distance between the anchor and the matching sample while maximizing the distance with the non-matching picture.\nBut Adalynn didn’t invent this function.\nWhich of the following is the name of the loss function that Adalynn is using?\n\nAdalynn is using a Contrastive loss function\nAdalynn is using a Binary Cross-entropy loss function\nAdalynn is using a Categorical Cross-entropy loss function\nAdalynn is using a Triplet loss function\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n4\nAdalynn is using Triplet loss, helpful in similarity learning problems where the goal is to learn a function that measures how similar two objects are.\nThe Triplet loss takes the three inputs: the anchor, a positive sample, and a negative sample, and minimizes the anchor—positive distance while maximizing the anchor—negative distance.\nThe Triplet loss function compares a baseline (the anchor) to positive and negative inputs. It then reduces the distance between the baseline and the positive sample while increasing the distance between the anchor and the negative input.\nA popular application of Triplet loss is to teach a model to detect faces. Instead of framing this use case as a classification problem, we can look at it as a similarity learning problem. The goal would be for a network to compare two pictures and output whether they are similar enough.\nNotice that we also use Contrastive loss in similarity learning problems, but this function receives two inputs and whether they represent the same concept or not.\nRecommended reading\n\n“What is Triplet loss” is a short introduction to this loss function.\nThe Wikipedia page of the Triplet loss function is also a great reference.\nCheck “Losses explained: Contrastive Loss” for an explanation of the Contrastive loss."
  },
  {
    "objectID": "20220915.html",
    "href": "20220915.html",
    "title": "The gradient of a function",
    "section": "",
    "text": "But to make a career in machine learning, you need at least to know some of the underpinnings behind specific techniques.\nLet’s pick neural networks, for example. The whole idea of optimizing the network rests on understanding how to compute the gradient of a function.\nWhich of the following is true about the gradient of a continuous and differentiable function?\n\nThe gradient of a continuous and differentiable function is zero at a minimum value.\nThe gradient of a continuous and differentiable function is zero at a saddle point.\nThe gradient of a continuous and differentiable function is non-zero at a maximum value.\nThe gradient of a continuous and differentiable function decreases as it gets closer to a minimum value.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 2\nTo keep it simple, the gradient of a function gives us the slope at any point on its curve. We can compute it by dividing the “rise” of the function by its “run” over a specific small interval.\nImagine the Sine function. Pick a point on the left side, and think of the tangent to the curve as you move the point closer to the maximum value of the function. The gradient starts positive and decreases as we get closer to the maximum.\nRepeat this exercise, starting from the maximum value and moving towards the minimum. The gradient is negative after the maximum point and increases until it reaches the minimum value. Visualizing this in your head is not easy, so here is an excellent interactive chart where you can see how this works using a similar function.\nWhat happens with the gradient when we get to a minimum, maximum, or saddle point? The gradient will equal zero if the curve has a horizontal slope at a particular point. That means that both the first and second choices correctly describe the properties of the gradient function. However, the third choice is invalid: we already know that the gradient function is zero when we hit a maximum point.\nRecommended reading\n\nCheck “Is the gradient function increasing or decreasing on this curve?” for a great visualization of the gradient of a function.\nThe Wikipedia pages of “Gradient” and “Saddle point” contain all the information you need to answer this question."
  },
  {
    "objectID": "20220820.html",
    "href": "20220820.html",
    "title": "Abigail’s chart",
    "section": "",
    "text": "She was ready to share her model with the rest of her team. This was her first machine learning experience, and she was proud of how much progress she had made in the last few weeks.\nAbigail plotted her model’s training and validation loss over the number of iterations (epochs). The cart looked like this:\n\nWhich of the following is the correct conclusion from looking at Abigail’s plot?\n\nThe training loss decreases continuously, indicating that the model is doing well.\nThe validation loss decreases to a point and starts increasing again, indicating the model is overfitting.\nThe training and validation loss suddenly slowed down, indicating a problem with the learning process.\nThe validation loss doesn’t follow the training loss closely, indicating the model is underfitting.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nAnalyzing learning curves is one of the fundamental skills you should build in your career. There are different learning curves, but we will focus on Abigail’s chart.\nFirst, notice that the chart shows the loss—or error—as we increase the number of training iterations. A good mental model is to look at this the following way: “as we keep training, how much better the model gets?” Since we are displaying the loss, larger values are worse, so having both lines decrease is a good sign.\nWe have two lines in the chart: one representing the loss we get during training, the other representing the loss during the validation process. How these lines look concerning each other is essential. Most of the time, one of the lines alone wouldn’t give you a complete picture of the situation.\nLet’s start with the first choice that argues that a training loss that’s continually decreasing is good. Indeed this could be a good sign in isolation: the more we train our model, the fewer mistakes we make on the training dataset, but this is not enough to draw any conclusions.\nAlways be suspicious of an always-decreasing training loss; it’s a sign that your model might be memorizing the data. Usually, you want a model that learns up to a point, and then the loss stays flat. But how do you know when during the process that should happen? The relationship between the training loss and the validation loss shows how they start diverging at about 100 iterations. This point is the key.\nThink about it this way: Abigail’s model “continues learning” the training data but stops learning the validation data at about 100 iterations. The model is overfitting: it’s memorizing the training data, which is not helping with the validation data. Therefore, the first choice is incorrect, and the second is correct.\nThe third choice is incorrect as well. You’ll always see a similar slow down in the loss functions. The best case—not usual, but technically possible—is to reach 0, where there’s nowhere else to go.\nFinally, the fourth choice is also incorrect. First, the training and validation loss do not necessarily need to follow each other. Second, this model shows overfitting—memorizing the training data—and not underfitting—lack of model complexity to learn the data correctly. You can usually identify an underfitting model when its training loss is flat or doesn’t decrease much.\nIn summary, the second choice is the correct answer to this question.\nRecommended reading\n\n“How to use Learning Curves to Diagnose Machine Learning Model Performance” is the article that inspired this question and from where I borrowed Abigail’s chart.\nWikipedia’s introduction to Learning curves."
  },
  {
    "objectID": "20220827.html",
    "href": "20220827.html",
    "title": "Predictive maintenance",
    "section": "",
    "text": "After successfully testing the model in a pilot project, the company plans to start using it in one of its warehouses. Before they are ready, Miriam needs to summarize the performance of the pilot project.\nShe ran pictures of 100 different components through the model and found the following:\n\nThe model predicted that 7 components needed maintenance. After manually inspecting them, only 5 required work, but the other 2 were okay.\nThe model predicted that 93 components were working as expected but missed 2 that needed maintenance.\n\nWhich of the following is the correct summary of accuracy, precision, and recall for Miriam’s model?\n\nThe model’s accuracy is 93%, the precision is 29%, and the recall is 71%.\nThe model’s accuracy is 98%, the precision is 71%, and the recall is 29%.\nThe model’s accuracy is 90%, the precision is 98%, and the recall is 29%.\nThe model’s accuracy is 96%, the precision is 71%, and the recall is 71%.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n4\nThe easiest way to answer this question is to put the information we know in a confusion matrix. Whenever you build a classification model, the confusion matrix will help tremendously. Let’s see how.\nHere is the confusion matrix with Miriam’s results. Notice the small annotations next to each value:\n\n\nMiriam’s model found 5 true positives (TP). These are the components that the model predicted that needed maintenance that actually needed it.\nThere are 2 false positives (FP). These are the other 2 components the model thought erroneously needed maintenance.\nThere are 91 true negatives (TN). These are the components the model classified as okay and were indeed fine.\nThere are 2 false negatives (FN). These are the 2 components that the model missed as needing maintenance.\n\nWe can compute the three metrics we need using this information. Let’s start with accuracy:\naccuracy = (TP+TN)/(TP+TN+FP+FN)\naccuracy = (5+91)/(5+91+2+2)\naccuracy = 96/100\naccuracy = 0.96\nThe precision of the model is the fraction of components that truly need maintenance among the components that the model predicts need it. We can compute the precision this way:\nprecision = TP/(TP + FP)\nprecision = 5/(5 + 2)\nprecision = 5/7\nprecision = 0.71\nThe recall of the model is the fraction of components that truly need maintenance among all the existing components that need maintenance. We can compute the recall this way:\nrecall = TP/(TP + FN)\nrecall = 5/(5 + 2)\nrecall = 5/7\nrecall = 0.71\nAs you can see, the fourth choice is the correct answer to this question.\nRecommended reading\n\nHere is the Wikipedia page explaining what a confusion matrix is. Most of the information you need is here.\nCheck out “Precision and recall” for a better understanding of these two metrics.\n“Evaluating Deep Learning Models: The Confusion Matrix, Accuracy, Precision, and Recall” is a great article that puts all of these concepts together."
  },
  {
    "objectID": "20220917.html",
    "href": "20220917.html",
    "title": "Output activation",
    "section": "",
    "text": "Alina wanted to do that. She wasn’t confident in understanding classification problems and neural networks, so she tried a few ideas. She implemented a neural network from scratch, including different loss functions and the backpropagation process.\nIt was time to look into activation functions for the output layer of the network, and here is where Alina had the most questions.\nWhich of the following activation functions could work in the output layer of a neural network that solves a classification problem?\n\nRectifier linear activation function (ReLU)\nTanh\nf(x) = 1 if x > 0 else 0\nf(x) = 0 if x > 0 else 1\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2, 3, 4\nWhenever we are working on a classification task, we need the network to output a finite range of values. Alina can use any of the choices on this question except ReLU.\nReLU returns its same input if positive or zero otherwise. Assuming the input to an output layer using ReLU is a positive value, the result will be the same, which won’t help Alina classify the sample.\nThe other three choices will help Alina classify images in one of two classes. Tanh will return a result between -1 and 1, so she can decide whether the image belongs to one class if the output is negative or the other if positive. The other two functions return a discrete value depending on their input.\nNotice that, although not strictly required, we usually prefer activation functions to be differentiable. While tanh is differentiable, the other three options aren’t. However, just like ReLU, this question’s third and fourth functions are differentiable except at x = 0, which is enough for them to work without too much trouble.\nRecommended reading\n\n“A Gentle Introduction to the Rectified Linear Unit (ReLU)” is a great introduction to ReLU.\nCheck out “Neural Network Binary Classification With Tanh Output Activation” for the source code of a binary classifier using a tanh activation function."
  },
  {
    "objectID": "20220821.html",
    "href": "20220821.html",
    "title": "Low-bias models",
    "section": "",
    "text": "High bias models are simpler to interpret and usually run fast, but they don’t scale well to complex problems.\nJuliette has already tried a few high-bias algorithms on her dataset and pushed them as far as possible. She is now ready to try something different.\nWhich of the following algorithms are low-bias models that Juliette can try on her problem?\n\nLinear Regression\nDecision Trees\nLogistic Regression\nk-Nearest Neighbors (KNN)\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2, 4\nEvery machine learning algorithm deals with three types of errors: bias, variance, and irreducible error. We need to focus specifically on the bias error to answer this question.\nHere is what Jason Brownlee has to say about bias: “Bias are the simplifying assumptions made by a model to make the target function easier to learn.”\nIn other words, “bias” refers to the assumptions the model makes to simplify the process of finding answers. The fewer assumptions it makes, the less biased the model is.\nNonlinear models are usually low-bias. They don’t make too many assumptions about the target function, making them an excellent option for tackling complex problems. Decision Trees and k-Nearest Neighbors are examples of low-bias models.\nOn the other hand, linear models are usually high-bias. They are easier to understand but make too many assumptions about the target function, preventing them from performing well on complex problems. Linear and logistic regression are two examples of high-bias models.\nRecommended reading\n\nHere is Jason Brownlee’s article I mentioned before: “Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning”.\nThe Wikipedia page on bias and variance is also a good resource: “Bias–variance tradeoff”."
  },
  {
    "objectID": "20220921.html",
    "href": "20220921.html",
    "title": "Vanishing gradients",
    "section": "",
    "text": "She is building a deep neural network but can’t even get it to train properly.\nKaylee mainly uses sigmoid as the activation function on the network’s layers, so a colleague mentioned the vanishing gradient problem as a potential cause.\nAssuming Kaylee’s colleague is correct, which of the following statements should help the network learn?\n\nKaylee should use tanh instead of sigmoid because it doesn’t suffer from vanishing gradients.\nKaylee should use ReLU instead of Sigmoid because it doesn’t suffer from vanishing gradients.\nKaylee should modify her model and introduce Batch Normalization to mitigate the problem.\nKaylee should initialize the network weights with large values to avoid vanishing gradients.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2, 3\nIf the gradients of the loss function approach zero, the model will stop learning because the network will stop updating the weights. This phenomenon is known as the Vanishing Gradient Problem, and it’s common when using the sigmoid and tanh activation functions in deep neural networks. Kaylee won’t fix the problem by replacing sigmoid with tanh.\nIn contrast, ReLU is a way to solve the vanishing gradient problem. ReLU is much less likely to saturate, and its derivative is 1 for values larger than zero, so Kayle should look at it to replace sigmoid.\nBatch normalization is another way to mitigate the vanishing gradient problem. Suppose we have a layer that uses a sigmoid activation function. We can normalize the input to that layer to ensure that the values don’t reach the edges and stay around the area where derivatives aren’t too small. By modifying the input to this layer with batch normalization, Kaylee will mitigate the vanishing gradient problem.\nFinally, while Kaylee may suffer from the vanishing gradient problem if she uses small values to initialize the network’s weights, she will have the opposite problem (exploding gradients) if she initializes the network with large values. “Initializing neural networks” is an excellent introduction to the importance of correct weight initialization.\nRecommended reading\n\nCheck “How to Fix the Vanishing Gradients Problem Using the ReLU” for recommendations on how to fix the vanishing gradient problem.\n“Initializing neural networks” is an excellent summary of the importance of weight initialization."
  },
  {
    "objectID": "20220924.html",
    "href": "20220924.html",
    "title": "The output of the network",
    "section": "",
    "text": "Although most of it needed experimentation, one thing Juniper could count on was the design of the output layer of the network.\nWhich of the following is a correct statement about the neurons in the output layer of a classification network?\n\nThe number of neurons in the output layer should always match the number of classes.\nThe number of neurons in the output layer doesn’t necessarily need to match the number of classes.\nThe number of neurons in the output layer should always be greater than one.\nThe number of neurons in the output layer should always be a multiple of 2.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nWhen working on a multi-class classification problem, setting the output layer to the same number of classes we are interested in predicting is common. But what happens when we are working on a binary classification problem?\nA network to solve a binary classification problem doesn’t need an output with two neurons. Instead, we can use a single neuron to determine the class by deciding on a cutoff threshold. For example, the result could be positive if the output exceeds 0.5 and negative otherwise.\nThat means we can solve a problem requiring two classes with a single neuron.\nWe can stretch the same idea to multi-class classification problems: We could interpret the output layer as a binary result, allowing us to represent multiple classes with fewer neurons. For example, we would need only two neurons to classify instances into four different categories (00, 01, 10, 11.) This approach, although not popular, it’s possible.\nTherefore, the second choice is the correct answer to this question.\nRecommended reading\n\n“But what is a neural network?” is Grant Sanderson’s introduction to neural networks on YouTube. Highly recommended!\nNeural Networks and Deep Learning is a free online book written by Michael Nielsen."
  },
  {
    "objectID": "20221009.html",
    "href": "20221009.html",
    "title": "Scikit-Learn’s fan",
    "section": "",
    "text": "She hasn’t used it for too long, but every time she learns something new, there’s always an elegant way to do it using Scikit-Learn.\nBut today is perhaps the exception: Melody finished training a multi-class classification model, and after displaying Scikit-Learn’s classification report, she noticed something wasn’t right.\nShe has to analyze the Macro-average, Micro-average, and Weighted F1-Score values for her model, but the classification report doesn’t display the Micro-average F1-Score.\nHow can Melody move forward?\n\nThe Micro-average F1-Score is the same as the model’s accuracy. Melody can run her analysis using the accuracy of her model.\nThe Micro-average F1-Score is the same as the model’s precision. Melody can run her analysis using the precision of her model.\nThe Micro-average F1-Score is the same as the model’s recall. Melody can run her analysis using the recall of her model.\nMelody will need to compute the Micro-average F1-Score of her model manually.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1\nWhen we work on multi-class classification problems, it’s simple to compute the F1-Score for each class. Several strategies exist to calculate a global F1-Score representing the entire model’s performance: Macro-average F1-Score, Micro-average F1-Score, and Weighted F1-Score.\nScikit-Learn’s classification report shows all three of these metrics but uses a different name for the Micro-average F1-Score. That’s why Melody is confused.\nThe Micro-average F1-Score is a metric where we sum all of the contributions from each category to compute an aggregated F1-Score. Micro-average F1-Score calculates the proportion of correctly classified samples out of all samples, which is the model’s accuracy definition.\nIn any multi-class classification problem, the Micro-average F1-Score is the same as the accuracy. That’s why Scikit-Learn displays it under the “accuracy” label.\nRecommended reading\n\n“Micro, Macro & Weighted Averages of F1 Score, Clearly Explained” is a great article covering how to compute a global F1-Score metric in multi-class classification problem.\nCheck “What is the F-Score?” for a short introduction to the Fβ-Score."
  },
  {
    "objectID": "20220912.html",
    "href": "20220912.html",
    "title": "Broken bottles",
    "section": "",
    "text": "Most of the bottles that come out are “ready,” and there are only a few samples that classify as “almost ready” or “waste.” Parker is very aware of this imbalance.\nThe factory uses Parker’s model to reduce the number of defective bottles that ship to customers (any bottle that’s not “ready” is defective.) They consider each of the three classes equally important and wants to ensure the evaluation process reflects that.\nWhich metric should Parker use to evaluate her model?\n\nThe accuracy of the model.\nThe Micro-average F1-Score of the model.\nThe Macro-average F1-Score of the model.\nThe Weighted F1-Score of the model.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n3\nSince Parker is working on a very imbalanced problem, she shouldn’t use accuracy to evaluate her model’s quality. If 99% of the bottles are “ready,” Parker’s model will have 99% accuracy, even if she ignores every sample belonging to the other two classes.\nThe F1-Score is helpful in these situations, but it depends on how we compute it in a multi-class classification problem.\nThe Micro-average F1-Score calculates the proportion of correctly classified samples out of all instances, the same as the model’s accuracy definition. Therefore, if we use the Micro-average F1-Score, we’ll have the same issue we see when using accuracy.\nThe Weighted F1-Score scales the individual F1-Score of each class. This method favors the majority classes, which in Parker’s case are the bottles classified as “ready.” The factory considers every category equally important, and that’s why the Weighted F1-Score is not the correct answer.\nFinally, the Macro-average F1-Score penalizes the model equally for any class that doesn’t perform well, regardless of its importance or how many support samples it has. This metric is the correct answer where every category is important, irrespective of how many instances we have in the dataset. This is the metric that Parker should use to evaluate her model.\nRecommended reading\n\nCheck “When accuracy doesn’t help” for an introduction to precision, recall, and f1-score metrics to measure a machine learning model’s performance.\n“Micro, Macro & Weighted Averages of F1 Score, Clearly Explained” is a great article covering how to compute a global F1-Score metric in multi-class classification problem.\nCheck “Confusion Matrix” for a full explanation of how a confusion matrix works and how you can use them as part of your work."
  },
  {
    "objectID": "20220906.html",
    "href": "20220906.html",
    "title": "Cheating with a model",
    "section": "",
    "text": "The professor decided to review her process step by step. Skylar stood in front of the class and wrote down her process:\n\nReplaced one column’s missing values with the mean of the column and scaled another column using Mix-Max Scaling.\nSkylar loaded the entire dataset in memory.\nShe then split the dataset into a train and a test set.\nAnd finally, she trained her model.\n\nAs soon as she finished, the professor knew what the issue was.\nWhich of the following is the reason for the model’s inflated performance?\n\nSkylar loaded the entire dataset in memory. She should have loaded it in batches.\nSkylar transformed her data before splitting the dataset. She should have split the data before transforming it.\nSkylar used the mean of the column to impute missing values. She should have used the median instead.\nSkylar used Min-Max Scaling to transform one of the columns. She should have applied log transformation instead.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nRule of thumb: When your model’s results seem too good to be true, they probably are.\nThe problem with Skylar’s model is a data leak: she transformed her dataset before splitting it. When computing the mean of a column or applying Min-Max Scaling, Skylar used the test data, which she isn’t supposed to have.\nIt’s subtle but essential.\nWhen doing any work with your data, ensure you only look at your train set. Test data is akin to production data: you don’t have it during training time and can’t make any decisions based on it.\nIn this example, Skylar imputed missing values using the mean of the entire column, including those samples that later became part of the test set. She also used Min-Max Scaling, which uses the minimum and maximum values of the column. She leaked information from the soon-to-be test dataset into her training process in both cases.\nThis leak caused Skylar’s model to perform much better than others. She cheated without realizing it.\nThe solution is to split the dataset before doing any transformations. Skylar should compute the training data’s mean, minimum, and maximum values and use them to impute and scale the test set.\nRecommended reading\n\nCheck Data Leakage in Machine Learning for an introduction to data leaks and how to prevent them.\nThe Wikipedia page on leakages in machine learning covers the topic very well."
  },
  {
    "objectID": "20220916.html",
    "href": "20220916.html",
    "title": "True Positives",
    "section": "",
    "text": "Assume that class B represents the outcomes of the model we are interested in finding.\nWhat’s the total of True Positives on this evaluation round?\n\nTrue Positives are class A samples the model predicted as class B, so the answer is 7.\nTrue Positives are class B samples the model predicted as class A, so the answer is 13.\nTrue Positives are class A samples the model predicted as class A, so the answer is 52.\nTrue Positives are class B samples the model predicted as class B, so the answer is 28.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n4\nWe are interested in samples from class B, which means we will treat class B as our “Positive” samples and class A as our “Negative” samples.\nIf we replace the classes in the confusion matrix with Positive and Negative instead of “B” and “A,” it’s much easier to reason about the model’s number of True Positives:\n\nTrue Positives are those samples that we expect to be Positive (class B), and the model predicted as Positive (class B.) Therefore, the correct answer to the question is 28. You can see every combination in the image above.\nRecommended reading\n\nCheck “Confusion Matrix” for a full explanation of how a confusion matrix works and how you can use them as part of your work.\nCheck “When accuracy doesn’t help” for an introduction to precision, recall, and f1-score metrics to measure a machine learning model’s performance."
  },
  {
    "objectID": "20220905.html",
    "href": "20220905.html",
    "title": "Zombies taking over",
    "section": "",
    "text": "But there was still hope.\nAdeline was part of the team dedicated to finding a cure to transform the zombies back. She wasn’t a machine learning expert but knew how to build logistic regression classifiers. Her job was to categorize zombies based on data captured by field teams.\nAssuming that there are five categories of zombies and that Adeline is only working with logistic regression models, how many models does she need to build to classify zombies correctly?\n\nAdeline can classify every zombie correctly by building a single model.\nAdeline can classify every zombie correctly by building 5 models.\nAdeline can classify every zombie correctly by building 24 models.\nAdeline cannot classify every zombie using logistic regression. She needs a multi-class classification model instead.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nThis is what we were aiming for:\nAdeline can solve the problem using logistic regression alone.\nLogistic regression estimates the probability of an event occurring by outputting a single value bounded between 0 and 1. The closer the result is to zero, the less likely the event will occur, while the closer is to 1, the more likely it is. This structure makes logistic regression ideal for tackling binary or 2-class classification problems. Notice that we can formulate a binary classification task as a 2-class classification task and vice versa.\nAdeline needs to classify zombies into five different categories. She can use the One-vs-All method (also called “One-vs-Rest.”) She will need to train five models:\nModel 1: Identifying Class 1 vs. [Class 2, Class 3, Class 4, Class 5]\nModel 2: Identifying Class 2 vs. [Class 1, Class 3, Class 4, Class 5]\nModel 3: Identifying Class 3 vs. [Class 1, Class 2, Class 4, Class 5]\nModel 4: Identifying Class 4 vs. [Class 1, Class 2, Class 3, Class 5]\nModel 5: Identifying Class 5 vs. [Class 1, Class 2, Class 3, Class 4]\nAfter running the data through every one of these five models, the correct category would be the prediction from the model with the highest confidence.\nRecommended reading\n\nCheck out “Logistic Regression for Machine Learning” for an introduction to Logistic regression.\n“Essential Data Science Tips: How to Use One-Vs-Rest and One-Vs-One for Multi-Class Classification” is a great reference to understand how to use the One-vs-All method for multi-class classification."
  },
  {
    "objectID": "20220911.html",
    "href": "20220911.html",
    "title": "Inattentive drivers",
    "section": "",
    "text": "Her first assignment is to build a model that will use the camera in the cabin to detect whether drivers are paying attention or are too tired.\nThe system’s ultimate goal is to provide audio and visual alerts to help drivers stay safe. Tesla hopes this feature will significantly decrease the number of accidents.\nBefore the work starts, Summer wants to decide which of the following will be the best way to evaluate the model:\n\nSummer should use the recall of the model, while keeping an eye on its precision.\nSummer should use the accuracy of the model.\nSummer should use the precision of the model, while keeping an eye on its recall.\nSummer should use the training loss of the model.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1\nThere will be many more situations where Summer’s system will find that the driver is correctly paying attention than not. If she uses the accuracy of her model as the evaluation metric, she could end up with a struggling model that still shows high accuracy. Remember that accuracy is not a good metric when facing an imbalanced problem. You can achieve very high accuracy even with a model that does nothing useful.\nLet’s assume Summer’s model misses most cases where the driver is tired and not paying attention. Think of a model that detects a single issue correctly, doesn’t have any false positives, and ignores all other dangerous situations. That model will have perfect precision, but it won’t help much in reducing traffic accidents because it doesn’t detect every problematic situation. Precision is not a good metric for this problem.\nThe training loss of the model won’t help Summer either. We use the loss to determine whether the model learned, but it doesn’t tell us anything about the capacity of the model to detect inattentive drivers. Like accuracy, we could have a low loss model that doesn’t do well with problematic situations.\nSummer should focus on the recall of her model while keeping an eye on its precision. A high recall will ensure that Summer’s model detects as many problematic situations as possible.\nRecommended reading\n\nCheck “When accuracy doesn’t help” for an introduction to precision, recall, and f1-score metrics to measure a machine learning model’s performance.\nCheck “Confusion Matrix” for a full explanation of how a confusion matrix works and how you can use them as part of your work."
  },
  {
    "objectID": "20221002.html",
    "href": "20221002.html",
    "title": "Useless explanation",
    "section": "",
    "text": "Those were the instructions that Brad sent Angelina. “I hate him so much,” she couldn’t stop thinking.\nAngelina’s dataset had a lot of columns, and she couldn’t fit it all into memory. Removing some of the columns was an excellent place to start, and since Brad had solved the problem already, she asked him.\n“But of course” —she kept thinking— “he couldn’t have been more useless.”\nAngelina doesn’t know the difference between ordinal and nominal columns.\nWhich of the following is the way forward for Angelina?\n\nAngelina should remove every categorical feature whose values have a meaningful order.\nAngelina should remove every categorical feature whose values don’t have a meaningful order.\nAngelina should remove every numerical feature but leave categorical features.\nAngelina should remove every categorical feature but leave numerical features.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1\nBoth ordinal and nominal features refer to categorical columns.\nOrdinal columns have a meaningful order. For example, you could have a feature representing a person’s economic status with three possible values: “low,” “medium,” and “high.” Notice how there’s a clear order among these values: “low” comes first, then “medium,” and finally “high.”\nCompare this with a nominal variable that doesn’t have a meaningful order between values. For example, a feature representing a “color” will not have any discernible order.\nRecommended reading\n\nCheck “Nominal, ordinal, or numerical variables?” to understand the difference between different types of features."
  },
  {
    "objectID": "20220913.html",
    "href": "20220913.html",
    "title": "Job descriptions",
    "section": "",
    "text": "Unfortunately, there’s a lot of noise in the dataset, and Millie is not sure how to proceed.\nA colleague recommended bagging, but Millie is unfamiliar with this technique. She looked into it and came up with a few questions.\nSelect every statement below that’s correct about bagging:\n\nBagging is an effective technique to reduce the variance of a model.\nBagging is an effective technique to reduce the bias of a model.\nBagging trains a group of models, each using a subset of data selected randomly without replacement from the original dataset.\nBagging trains a group of models, each using a subset of data selected randomly with replacement from the original dataset.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 4\nBagging is a popular ensemble learning technique.\nBagging trains a group of models in parallel and independently from each other. Each model uses a subset of the data randomly selected with replacement from the original dataset. That’s why bagging is also known as “bootstrap aggregating:” because it draws bootstrap samples from the training dataset.\nBagging is an excellent approach to reducing the variance of a model, but it doesn’t help reduce the bias. That’s why we often use it with low-bias models, like unpruned decision trees. Here is a relevant quote from “What is bagging?” about how it helps reduce the variance of a model:\n\nBagging can reduce the variance within a learning algorithm. This is particularly helpful with high-dimensional data, where missing values can lead to higher variance, making it more prone to overfitting and preventing accurate generalization to new datasets.\n\nIn summary, the first and fourth choices answer this question correctly.\nRecommended reading\n\nCheck IBM’s “What’s Bagging?” summary for a detailed explanation of this technique.\nWhat is the difference between Bagging and Boosting? is a great summary of bagging and boosting and their advantages and disadvantages."
  },
  {
    "objectID": "20221007.html",
    "href": "20221007.html",
    "title": "Features and data",
    "section": "",
    "text": "Which of the following statements correctly summarizes your thoughts about the relationship between features and dataset size?\n\nWhen training a model, as you add more features to the dataset, you often need to increase the dataset’s size to ensure the model learns reliably.\nWhen training a model, adding more features to the dataset increases the amount of information you can extract, allowing you to use smaller datasets and still extract good performance from the data.\nWhen training a learning algorithm, as you decrease the number of features in your dataset, you need to increase the number of training samples to make up the difference.\nWhen training a learning algorithm, the features in your dataset are entirely independent of the number of training samples.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1\nRemoving features reduces the number of dimensions in our data. It concentrates the samples we have in a lower-dimensional space. We can’t replace the information provided by a feature with more data.\nImagine plotting a set of numbers. Since you have only one dimension, they will all lie somewhere in a line. Don’t add new values; increase the features by adding a second dimension. Your values now become a set of 2D coordinates (x, y); if you graph them, they will all be somewhere in a plane.\nIf you compare the 1D line with the 2D plane (or even a 3D space, assuming you add a third dimension,) something will become apparent quick: As we increase the dimensionality of the data, it will be harder and harder to fill up the space with the same points.\nThis increase in sparsity will make it much harder for the learning algorithm to find interesting patterns.\nBased on this, we can conclude that there’s a relationship between features and samples, and the more features we add, the more data points we need.\nThe first choice is the only correct solution to this question. For a more formal definition, look at the Curse of Dimensionality: The amount of data needed to extract relevant information increases exponentially with the number of features in your dataset.\nRecommended reading\n\nCheck “What is the Curse of Dimensionality?” for an introduction to the Curse of Dimensionality."
  },
  {
    "objectID": "20221004.html",
    "href": "20221004.html",
    "title": "Two experiments. Round one.",
    "section": "",
    "text": "One of the experiments used a very small batch size, and the other used a batch size equal to all existing training data.\nAfter training and evaluating her models, Lena plotted the training and testing losses over 100 epochs of one of the experiments, and this is what she got:\n\nWhich of the following options is the most likely to be true?\n\nThe model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took less time to train.\nThe model behind this plot corresponds to the experiment that used a very small batch size. This model was the one that took longer to train.\nThe model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took less time to train.\nThe model behind this plot corresponds to the experiment that used a batch size equal to all the available training data. This model was the one that took longer to train.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n3\nThe amount of noise in the plot is crucial to answering this question.\nThe lower the batch size, the more noise we will get in the model’s loss. When we only use a few samples, any instances that vary dramatically will cause the loss to swing wildly. When we use a larger batch, no individual sample would have the power to sway the loss too much, so we should expect less noise.\nThis plot shows a very smooth loss with almost no noise, which is likely the experiment that uses the entire training set during every epoch.\nThe smaller the batch size, the more times we will need to compute the loss and update the model’s weights during backpropagation. For example, if Lena’s training set has 800 samples and uses one as her batch size, the algorithm will update the model’s weights 800 times. However, if she uses a batch size equal to the size of her training set, the algorithm will update the model’s weights 1 time.\nUpdating the model during every iteration is computationally expensive, so using a larger batch size will usually take less time. Therefore, the third option is probably the correct answer to this question.\nRecommended reading\n\n“The wrong batch size is all it takes” explains how different batch sizes influence the training process of neural networks using gradient descent.\nCheck “An overview of gradient descent optimization algorithms” for a deep dive into gradient descent and every one of its variants."
  },
  {
    "objectID": "20220822.html",
    "href": "20220822.html",
    "title": "Three elevators",
    "section": "",
    "text": "The views were amazing, but the building was busy, and getting an elevator during peak hours was always a chore.\nIris’s floor had three elevators unequally spaced along a wall. On a dull afternoon, while waiting for their ride to come, Iris asked her friends where they would stand and wait for the elevator. She wanted to minimize the distance they had to walk when one of the elevators arrived.\nWhich of the following will minimize the distance they have to walk? Keep in mind the elevators are not spaced equally along the wall.\n\nThey should always stand in any of the elevators at both extremes of the wall.\nThey should always stand in front of the middle elevator, regardless of its location along the wall.\nThey should always stand at the mid-point between the two elevators located at the extreme.\nThey should only stand in front of the middle elevator if the distance to the other two elevators is the same. If not, they can stand in any of the extreme elevators.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nIf Iris stands in front of the middle elevator, she will minimize the distance she has to walk. This holds for any distribution of the elevators along the wall—they don’t need to be equally spaced.\nLet’s break this down into the three possible situations with an example where we have elevators A, B, and C located along a wall (paraphrasing the answer on this Stack Exchange page):\n\nIf Iris waits in front of elevator A and elevator B arrives, she will need to walk from A to B. If elevator C arrives, she must walk from A to C, passing B on her way. Here is the pattern: A→B or A→B→C.\nIf Iris waits in front of elevator C and elevator A arrives, she will need to walk from C to A, passing B on her way. If elevator B comes, she must walk from C to B. Here is the pattern: C→B→A, or C→B.\nIf Iris waits in front of elevator B and elevator A arrives, she will need to walk from B to A. If elevator C arrives, she must walk from B to C. She will never have to pass any other elevator. Here is the pattern: B→A or B→C.\n\nDo you see how standing in front of any of the elevators at both extremes of the wall will make you walk the same distance in multiple cases? A→B in the first case, and C→B in the second one.\n“Visualizing the Median as the Minimum-Deviation Location” explains this as follows:\n\nFirst, consider waiting at a median location. Then, consider “moving” to a waiting position away from this median point. If one does so, one moves away from more elevators than one moves towards, thereby increasing the sum of the distances to the elevators.\n\nA more exciting insight: we can minimize the distance by standing at the median point between elevators for any number of elevators, not only three. Notice that this median point is not necessarily in front of an elevator.\nNow that we know the correct answer is the second choice imagine if we wanted to minimize the total distance they have to walk from the door they enter the lobby where the elevators are located. It’s no longer about the distance between the elevators, but we also need to consider the distance from the door to the elevator that arrives. The answer may surprise you.\nRecommended reading\n\nTake a look at “Visualizing the Median as the Minimum-Deviation Location” for an in-depth analysis of this problem.\n“Why does minimizing the MAE lead to forecasting the median and not the mean?” is the question that led me down this rabbit hole."
  },
  {
    "objectID": "20221010.html",
    "href": "20221010.html",
    "title": "Learning gap",
    "section": "",
    "text": "As you can see, after finishing training, both losses decreased nicely, but there’s a large gap between the training and the testing curves.\nWhat’s a reasonable conclusion about this machine learning model?\n\nThis model is underfitting.\nThis model is well-fit.\nThe training dataset is not sufficiently large.\nThe training and testing datasets are different enough for the model to struggle to solve testing samples.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n3, 4\nA good model should capture valuable patterns in the data and discard any noise that doesn’t help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset.\nAn overfitting model should not have any problems with the training data but stumble with the testing data. Therefore, we should expect a low training loss and a high testing loss. An underfitting model should struggle with the training and testing datasets, so both of its losses should be high. A well-fit model, however, should have low training and testing losses.\nThe chart shows a model that, while making progress, can’t entirely solve the testing dataset. This happens whenever the training data is insufficient to train a model that performs well on unseen data.\nFor example, the training and the testing data might differ enough that the model can’t get many of the testing samples correct, or the training dataset might not be large enough.\nRecommended reading\n\nCheck “Overfitting and Underfitting with Learning Curves” for an introduction to two fundamental concepts in machine learning through the lens of learning curves."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Questions by bnomial",
    "section": "",
    "text": "This website is built with markdown and Quarto"
  },
  {
    "objectID": "20221013.html",
    "href": "20221013.html",
    "title": "Coffee beans",
    "section": "",
    "text": "To measure the model’s efficacy, Hadley decided to use the F1-Score.\nBut there’s one problem: Hadley knows how to compute the F1-Score for every class separately, but she needs a single F1-Score value to compare different model versions and choose the best one.\nSelect from the following list every metric that Hadley can use to summarize the F1-Score for her model?\n\nHadley can compute the Macro-average F1-Score.\nHadley can compute the Micro-average F1-Score.\nHadley can compute the ROC F1-Score.\nHadley can compute the Weighted F1-Score.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 2, 4\nWhen building a classification model, the precision and recall scores are two metrics that indicate how effective the model is. There’s a trade-off between these metrics, so higher precision models sacrifice recall and vice versa.\nIt’s hard to compare classifiers unless we have a single metric that summarizes the balance between precision and recall. The Fβ score lets us do that.\n\nWhen using β = 1, we place equal weight on precision and recall. For values of β > 1, recall is weighted higher than precision; for values of β < 1, precision is weighted higher than recall. F1-Score is the most commonly used, which is the Fβ score with β = 1.\nComputing the F1-Score for each class in a multi-class classification problem is simple: F1-Score = 2 × (precision × recall) / (precision + recall), but this leads to the problem Hadley is facing: How can she combine the individual F1-score values into a single F1-Score value?\nThe Macro-average F1-Score is one approach where we calculate the F1-Score for each category and then average all the results. This method penalizes the model equally for any class that doesn’t perform well, regardless of its importance or how many support samples it has.\nThe Micro-average F1-Score is another approach where we sum all of the contributions from each category to compute an aggregated F1-Score. In this case, we don’t use the individual F1-Scores but the overall precision and recall across all samples. This method doesn’t favor or penalize any class in particular.\nFinally, the Weighted F1-Score is another way of computing a global F1-Score. In this approach, we weigh every individual F1-Score using the number of true labels of each class and then sum them to produce the global F1-Score. This method favors the majority classes because they will be weighted more in the computation.\nThe ROC F1-Score is a made-up term and therefore is not correct.\nRecommended reading\n\n“Micro, Macro & Weighted Averages of F1 Score, Clearly Explained” is a great article covering how to compute a global F1-Score metric in multi-class classification problem.\nCheck “When accuracy doesn’t help” for an introduction to precision, recall, and f1-score metrics to measure a machine learning model’s performance.\nCheck “Confusion Matrix” for a full explanation of how a confusion matrix works and how you can use them as part of your work."
  },
  {
    "objectID": "20221012.html",
    "href": "20221012.html",
    "title": "Recurring daughter",
    "section": "",
    "text": "Instead, I recommended recurrent neural networks as a good starting point. Going to transformers is overkill, and I’d rather have her look into some foundational ideas first.\nWhich of the following do you think I should include in the list of differences between recurrent and traditional networks?\nTraditional neural networks can process inputs of any length. Recurrent neural networks require a fixed-size sequence as their input.\nRecurrent neural networks capture the sequential information present in the input data. Traditional neural networks don’t have this ability.\nRecurrent neural networks share weights across time. Traditional networks use different weights on each input node.\nRecurrent neural networks can consider future inputs to compute their current state. Traditional neural networks can’t access future inputs.\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2, 3\nRecurrent neural networks (RNN) are a type of artificial neural network that can process sequential or time series data. Their main difference from traditional networks is their ability to take information from prior inputs to influence the current input and output. This ability allows them to capture any sequential information present in the data. For example, an RNN is ideal for capturing the dependency between words of a sentence.\nRNN processes the data sequentially so a model can process sequences of varying sizes. For example, an RNN can process a 5-word and 10-word sentence using the same input structure, unlike a traditional neural network that will need a different input size for each case.\nRNNs share the same weight parameters for every input sample, unlike traditional networks with different weights across each input node. Sharing parameters helps an RNN generalize to sequences of varying lengths and operate similarly on sequences with the same meaning but organized differently. The accepted answer to this question expands on this topic with an example.\nFinally, neither recurrent nor traditional networks can access future inputs to compute their current state.\nRecommended reading\n\nCheck “Recurrent Neural Networks” for a description of what they are and how they work.\n“An Introduction To Recurrent Neural Networks And The Math That Powers Them” is a deeper dive into RNNs."
  },
  {
    "objectID": "20220904.html",
    "href": "20220904.html",
    "title": "Initializing neural networks",
    "section": "",
    "text": "But Gabriella didn’t have that luxury. Her research got her very deep into how neural networks work.\nAs part of the project, she wants to write a blog post about the influence of different initialization schemes on how neural networks learn. She wants to cover the problems that could happen when we use the wrong initialization.\nWhich of the following problems could we face if we don’t initialize the weights correctly?\n\nThe network might suffer from vanishing gradients, which could cause a slow down in training.\nThe network might suffer from exploding gradients, preventing it from learning correctly.\nEvery network neuron might learn the same features during training.\nEvery neuron of the network might learn different features during training.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 2, 3\nCorrectly initializing a neural network can have a significant impact on convergence.\nWe may suffer from the vanishing gradient problem if we use small values to initialize the network’s weights and the exploding gradient problem if we use large weight values instead. The gradients will become smaller or larger as we move from the output layer toward the input layer during backpropagation. “Initializing neural networks” illustrates a specific example with a 9-layer neural network using the identity function as the activation on every layer.\nThe third choice is correct: if we initialize the network with zeros, every neuron will learn the same features. Here is an excerpt from “Initializing neural networks” explaining the consequences of using the same weight values:\nThus, both hidden units will have identical influence on the cost, which will lead to identical gradients. Thus, both neurons will evolve symmetrically throughout training, effectively preventing different neurons from learning different things.\nThe final choice, however, is not a problem. We want every neuron to learn different features.\nRecommended reading\n\n“Initializing neural networks” is an excellent summary of the importance of weight initialization.\nCheck “Weight Initialization Techniques in Neural Networks” to learn about different initialization schemes."
  },
  {
    "objectID": "20220928.html",
    "href": "20220928.html",
    "title": "Six months of data",
    "section": "",
    "text": "She used six consecutive months of data: the first five months to train the model and the sixth to test it.\nAfter trying multiple times, Valerie’s model performance on the training data had a large gap with the performance on the test data. The model works well with the training dataset but struggles to keep up with the test data.\nWhich of the following could be valid reasons for this problem?\n\nValerie’s optimizer is incorrect for this particular problem.\nValerie is not using the correct loss function for this particular problem.\nValerie’s model doesn’t have enough complexity to capture the relevant signal from the data.\nThe training data does not come from the same distribution as the test data.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n4\nValerie is training her model on five consecutive months’ worth of data and testing it with another month worth of data. If something changed during that time, Valerie might have a test set that fundamentally differs from her training data.\nThis problem is common, and a way to approach it is to ensure the training and test data come from the same distribution. “Adversarial validation” is a clever technique you can use to accomplish this.\nNone of the other three options explain a model that works with the training data but struggles with the test dataset.\nRecommended reading\nCheck “Adversarial validation” for a quick introduction to this technique."
  },
  {
    "objectID": "20220829.html",
    "href": "20220829.html",
    "title": "Distance measures",
    "section": "",
    "text": "Every distance measure doesn’t work for every problem. Instead, we must select the appropriate function depending on the nature of the data.\nHere is a list of four distance measures and a quick summary of how they work. Select every one of them that’s correct.\n\nThe Euclidean distance between two vectors is the square root of the sum of the squared differences between them.\nThe Manhattan distance between two vectors is the sum of their absolute differences.\nThe Hamming distance is a generalization of the Euclidean and Manhattan distances that we can tune depending on which distance measure we need.\nThe Minkowski distance computes the distance between two binary vectors.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 2\nWhile the Euclidean and Manhattan distances summary is correct, the Hamming and Minkowski summaries aren’t.\nThe Hamming distance computes the distance between two binary vectors. For example, you can calculate the distance between objects using a one-hot encoded feature.\nThe Minkowski distance is a generalization of the Euclidean and Manhattan distances. Both of these work with real-value vectors, but the Euclidean distance is the shortest path between objects, while the Manhattan distance is the rectilinear distance between them. Using the Minkowski distance, we can control which approach to use depending on the data.\nRecommended reading\n\nCheck “4 Distance Measures for Machine Learning” for a complete explanation of these four distance measures.\n“Five Common Distance Measures in Data Science With Formulas and Examples” is a deeper dive into these distance measures."
  },
  {
    "objectID": "20220903.html",
    "href": "20220903.html",
    "title": "Recognizing overfitting",
    "section": "",
    "text": "She started training models recently and would love to know how to recognize when one of her models overfit.\nIf you were to summarize it for Cora, in which of the following situations is a model overfitting?\n\nThe training loss on the model stays constant during the entire training process.\nThe model’s performance on the training set is good but low on the validation set.\nThe model’s performance on the training and validation sets is low.\nThe model takes an extremely long time to converge.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nIf the training loss doesn’t move during the training process, something is wrong, but we can’t say that the model is overfitting. Overfitting is also not related to the speed of convergence. There may be multiple reasons a model takes a long time to converge, but we can’t claim that a model is not converging fast enough because it’s overfitting.\nWhen a model is overfitting, we should see a good performance on the training set alone. A good model should capture valuable patterns in the data and discard any noise that doesn’t help with predictions, leading to good performance in both the training and validation sets. The third choice argues for low performance on both training and validation sets, which will be a way to detect underfitting.\nIn summary, the second option is the correct answer: Cora should expect good performance on the training set but low validation performance.\nRecommended reading\n\nCheck “Overfitting and Underfitting With Machine Learning Algorithms” for an introduction to overfitting and underfitting in machine learning.\n“How to Solve Underfitting and Overfitting Data Models” covers several strategies to solve overfitting and underfitting."
  },
  {
    "objectID": "20220824.html",
    "href": "20220824.html",
    "title": "The political reporter",
    "section": "",
    "text": "In her short career as a political reporter, she has never dealt with technical data, so imagine her surprise when she realized the information her source sent her was not in plain English.\nThe hospital has been using a machine learning model to predict whether patients sick with COVID-19 will end up in the hospital. The report came with a bunch of numbers, and it measured the quality of the model using two weird terms: “precision” and “recall.”\nAria has to write her article, but first, she needs to figure out how to interpret the numbers.\nWhich of the following statements accurately represent what precision and recall are?\n\nPrecision: Among all the patients classified by the model as potential hospitalizations, the fraction that ended up in the hospital.\nPrecision: Among all the patients that ended up in the hospital, the fraction that the model correctly classified.\nRecall: Among all the patients classified by the model as potential hospitalizations, the fraction that ended up in the hospital.\nRecall: Among all the patients that ended up in the hospital, the fraction that the model correctly classified.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 4\nIn machine learning, especially when building classification models, precision and recall are two of the most critical metrics we need to understand.\nLet’s start with the definition from Wikipedia:\nPrecision is the fraction of relevant instances among the retrieved instances, while recall is the fraction of relevant instances that were retrieved.\nUsing this definition, we can break down what precision is by looking at two components:\n\nRelevant instances: These are all patients the model predicted as positive and actually ended up in the hospital. We call these “true positive” patients.\nRetrieved instances: These are all the patients the model predicted would end up in the hospital.\n\nThe precision is the number of relevant instances divided by retrieved instances. We want to express how “precise” the model was: out of every patient the model predicted would end up in the hospital, how many were actually hospitalized. Looking at the available choices, the first is the correct definition of precision.\nLet’s do the same with recall. We need two components:\n\nRelevant instances: These are all patients the model predicted as positive and actually ended up in the hospital. We call these “true positive” patients.\nPositive instances: The number of patients that ended up in the hospital.\n\nThe recall is the number of relevant instances divided by the number of positive instances. In other words, we want to express the capacity of the model to find positive patients: out of every patient that ended up in the hospital, how many did the model retrieve. Looking at the choices, the fourth is the correct definition of recall.\nIn summary, the first and fourth choices are the correct answer to this question.\nRecommended reading\n\nCheck out “Precision and recall” for a better understanding of these two metrics.\n“Precision vs Recall” is a great article covering the tradeoffs of precision and recall using a real-life example."
  },
  {
    "objectID": "20220818.html",
    "href": "20220818.html",
    "title": "Maximum depth",
    "section": "",
    "text": "But unfortunately, that’s not the only thing the classroom had to remember for their test. There were many more hyperparameters with funny names and responsibilities, and they barely had time to prepare for the exam.\nAs soon as Isabella looked at the first section of the test, her heart dropped. It was all about decision trees, and she spent most of her time focusing on neural networks.\nThe first question didn’t seem too tricky, however.\nWhich of the following is the correct definition of the maximum depth of a decision tree?\n\nThe maximum depth of a decision tree is the length of the longest path from the tree’s root to a leaf.\nThe maximum depth of a decision tree is the length of the shortest path from the tree’s root to a leaf.\nThe maximum depth of a decision tree is the length of the longest path from the tree’s root to an interior node.\nThe maximum depth of a decision tree is the length of the shortest path from the tree’s root to an interior node.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1\nIn a few words, a decision tree represents particular observations about a sample in its branches and the conclusions about that item’s target value as the tree’s leaves. If you draw the tree, its maximum depth is a way to explain how tall the tree is.\nIt makes sense that the maximum depth is the length of the longest path from the root to the bottom of the tree, but what exactly is that bottom? Is it a leaf or an internal node?\nYour decision tree would not be complete if you only considered internal nodes. Remember, leaves are where the conclusions of the target value live. In other words, a leaf is where the final answer lives. You can’t talk about the maximum depth of a tree without considering the leaves, so that’s the correct answer to this question.\nOne more important thing to notice is that everything else being equal, a more straightforward decision tree will always generalize better than a more complex one. The maximum depth of the tree directly affects its complexity.\nIn summary, the first choice is the correct answer to this question.\nRecommended reading\n\n“Decision tree learning” is the Wikipedia page where you can read about decision trees.\n“How to tune a Decision Tree?” is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth hyperparameter."
  },
  {
    "objectID": "20220830.html",
    "href": "20220830.html",
    "title": "Softmax mockery",
    "section": "",
    "text": "Alan was cooky. He didn’t know much but liked to show off with every bit of trivia that crossed his path. More than anything, he enjoyed pissing off Loretta at every opportunity he had.\nAnd here is Alan, bugging Loretta about the softmax function and ready to mock her if she fails.\nWhich of the following are correct statements about the softmax function?\n\nThe softmax function is a soft or smooth approximation to the max function.\nThe softmax function is a soft or smooth approximation to the argmax function.\nThe softmax function converts a vector of n values into a probability distribution of n possible outcomes.\nSoftmax is a differentiable function.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2, 3, 4\nThe name “softmax” is a misnomer. The softmax function is a smooth—soft—approximation of the argmax function, not the max function. To avoid this confusion, some of the literature uses “softargmax” instead, but the machine learning world ran with “softmax” and never looked back.\nThe softmax function turns a vector of n values into another vector of n probabilities that sum to 1. These probabilities are proportional to the relative scale of each value in the vector. For example, when we use softmax as the activation function of a neural network’s output layer, we get a normalized probability distribution that we can use to interpret the result of multi-class classification models.\nFinally, contrary to the argmax function, the softmax function is differentiable.\nRecommended reading\n\nCheck “The Softmax function and its derivative” for a complete explanation of softmax.\n“What is the Softmax Function?” is another great explanation of how the function works."
  },
  {
    "objectID": "20220927.html",
    "href": "20220927.html",
    "title": "Data augmentation on YouTube",
    "section": "",
    "text": "She wants to cover some of the most critical aspects of using the technique on a dataset of pictures.\nHere is Amara’s list with the key takeaways she wants to leave for her audience.\nWhich of the following statements would you let Amara share with her audience?\n\nUsing data augmentation, we can artificially increase the amount of data by generating new samples from existing data.\nGenerative Adversarial Networks (GANs) and Style Transfer are advanced techniques that can help with data augmentation.\nData augmentation has a regularization effect when used to increase the amount of data before training a model.\nData augmentation helps remove the inherent bias present in the original data.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 2, 3\nThe first choice is the definition of data augmentation: we can augment the size of the dataset by generating new samples from our existing data.\nGenerative Adversarial Networks (GANs) are a popular way to generate synthetic images. We can also use Style Transfer to create new data based on existing samples.\nThe third choice is also correct: Data augmentation has a regularization effect. Increasing the training data through data augmentation decreases the model’s variance and, in turn, increases the model’s generalization ability.\nFinally, when we use data augmentation, we will propagate any of the biases already present in the original data. Remember that data augmentation uses existing data as the foundation for any new data, so any problems with the original dataset will persist on the augmented one. Therefore, the fourth choice is incorrect.\nRecommended reading\n\n“STaDA: Style Transfer as Data Augmentation” is a paper illustrating how to use Style Transfer for data augmentation.\n“The Essential Guide to Data Augmentation in Deep Learning” is an excellent article discussing data augmentation in detail.\nCheck “Test-Time augmentation” for an introduction that will help you make better predictions with your machine learning model."
  },
  {
    "objectID": "20220925.html",
    "href": "20220925.html",
    "title": "Studying decision trees",
    "section": "",
    "text": "Following the recommendation of her peers, the first algorithm she looked into was decision trees. She found a lot of resemblance with some of the techniques she already knew.\nAfter a few weeks, Kehlani wants to summarize what she learned in an email to her team, but first, she wants you to review it.\nThese are the advantages that Kehlani listed. Which of them would you say are actual advantages of decision trees?\n\nDecision trees are simple to understand and interpret, and we can visualize them.\nUnlike other algorithms, many implementations of decision trees work with missing values and categorical data.\nDecision trees always generalize well and are resistant to overfitting.\nDecision trees require little data preparation compared to other algorithms. For example, they don’t need the scaling of data.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 2, 4\nDecision Trees are an excellent starting point for developers that want to learn machine learning.\nOne of the reasons decision trees are popular is because they are easier to understand and interpret than other algorithms. You can explain the output of a tree by boolean logic, which makes them transparent compared to algorithms we can’t explain. Kehlani did an excellent job by including this advantage in her email.\nMany different implementations of decision trees are very flexible with the data they can process. For example, some implementations can handle missing values and work directly with categorical data. Most machine learning algorithms don’t have this luxury and require a much more extensive pre-processing step before training a model.\nSomething similar happens with having to scale the data. Decision trees can handle features with different scales. For example, it can take a column representing a person’s age with values between 0 and 100 and another with a salary ranging from 20,000 to 500,000. Not every algorithm has this flexibility. Neural networks, for example, struggle when features don’t have the same approximate scale.\nDecision trees, unfortunately, are prone to overfitting if we don’t take careful care of their depth. In other words, unless we ensure our tree doesn’t go too deep, it will tend to fit noisy samples and output the wrong prediction. The only example where Kehlani made a mistake is the third option.\nRemember that while a single decision tree is prone to overfitting, using an ensemble of trees is more resistant. Here is a quote from “To Boost or not to Boost: On the Limits of Boosted Neural Networks”: “[these experiments] confirm that training single large decision trees is prone to overfitting while boosted ensembles of decision trees are resistant to overfitting.”\nRecommended reading\n\nThe Scikit-Learn’s “Decision Trees” page contains an extensive list of advantages and disadvantages of decision trees.\nCheck “To Boost or not to Boost: On the Limits of Boosted Neural Networks”, the paper cited above comparing the overfitting tendency of a single Decision Tree versus an ensemble of trees.\n“Decision tree pruning” covers the process of pruning a tree to reduce overfitting."
  },
  {
    "objectID": "20220817.html",
    "href": "20220817.html",
    "title": "A classification model",
    "section": "",
    "text": "A critical step she wants to take is introducing machine learning into her work. She started learning some of the fundamentals and is now ready to apply what she’s learned.\nShe researched one of her company’s problems and learned she needed to build a supervised learning classification model. She had enough labeled data, so it seemed like a good fit.\nBased on this, which of the following better describes what Natalie needs to accomplish?\n\nShe needs to train a model that returns a numerical prediction for each sample of data.\nShe needs to train a model that clusters the data into different groups based on their characteristics.\nShe needs to train a model to predict the class of every sample of data out of a predefined list of classes.\nShe needs to train a model that returns the optimal policy that maximizes the potential outcomes of her problem.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n3\nNatalie’s problem requires her to predict the class of every sample of data out of a predefined list of classes. That’s the goal of machine learning classification models.\nThe first choice refers to a regression model. Here we want the model to output a single, continuous value. For example, imagine we want to return the predicted price of a house or the predicted highest temperature for the weekend.\nThe second choice refers to a clustering model. These unsupervised learning techniques are helpful when we don’t have labels for our data and want the algorithm to group every sample into dynamically generated groups.\nThe fourth choice is a loose description of a reinforcement learning approach, where we want an agent to learn the optimal policy that maximizes a reward function.\nRecommended reading\n\n“4 Types of Classification Tasks in Machine Learning” is an excellent introduction to classification models in machine learning.\nFor an introduction to classification models, check “Classification in Machine Learning: What it is and Classification Models”"
  },
  {
    "objectID": "20220819.html",
    "href": "20220819.html",
    "title": "Hiring a team",
    "section": "",
    "text": "Jordan was one of the first. She came right off her Ph.D. to lead the creation of an automated data pipeline for the system. There was only one small problem: she didn’t have industry experience, and her research work was very different.\nJordan needs to build a team, but it’s hard to identify the appropriate candidates without fully understanding what goes into a data pipeline.\nWhich of the following are some of the data processing steps that go into a data pipeline?\n\nData analysis is one of the steps of a data pipeline.\nData ingestion is one of the steps of a data pipeline.\nData preparation is one of the steps of a data pipeline.\nData validation is one of the steps of a data pipeline.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2, 3, 4\nA critical part of any production-ready machine learning system is the data pipeline. Its goal is to collect and prepare the data we can use later to train, validate and use the model.\nA data pipeline is a fully automated process. There shouldn’t be any steps that require human intervention.\nAnalyzing the data is not part of a data pipeline. During the data analysis phase, the team will define the data required by the system. This is a manual step that happens before the data pipeline is ready. During the data analysis phase, the team will make the necessary decisions to build the data pipeline. Therefore, the first option is incorrect.\nData ingestion is one of the steps of a data pipeline. During this step, we ingest the data from the different data sources and transfer it to where we will store it and later process it. For example, the company may need data currently stored in spreadsheets, databases, and a few real-time sensors. The data ingestion step will get the necessary data from each place and keep them in the final location for the rest of the process.\nData preparation is another essential step of a data pipeline, where we format and prepare the data before using it. For example, the company might want to standardize the format of dates collected from different sources or perform specific transformations on numerical data.\nFinally, data validation is another step of a data pipeline. We can validate that the necessary data we are ingesting is present and follow the appropriate schema during this step. For example, we might need to collect a minimum number of samples daily and raise an exception during the data validation step if we don’t meet that condition.\nRecommended reading\n\nThe “Machine Learning Data Lifecycle in Production” course in Coursera, part of the Machine Learning Engineering for Production (MLOps) Specialization.\n“What is a Data Pipeline” is a short article explaining the high-level idea of data pipelines."
  },
  {
    "objectID": "20220918.html",
    "href": "20220918.html",
    "title": "Policing crime",
    "section": "",
    "text": "She wants the model to predict the areas where new violations are likely to occur so the department can reinforce the security around those streets.\nWhich of the following is a potential problem that Brielle should keep in mind?\n\nThere won’t be any reliable way to evaluate this model.\nThe model may suffer from survivorship bias.\nThe model may suffer from decline bias.\nThe model may create a positive feedback loop.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n4\nEvaluating this model doesn’t need to be complicated. Assuming that Brielle uses a Supervised Learning approach, she will have several options to assess the quality of the model predictions. Therefore, the first choice is incorrect.\nSurvivorship bias is when we concentrate on samples that made it past a selection process and ignore those that did not. Nothing in the problem statement indicates that Brielle’s model will suffer from this problem.\nDecline bias refers to the tendency to compare the past to the present, leading to the assumption that things are worse or becoming worse simply because change is occurring. The third choice is not a correct answer either.\nFinally, this model may create a positive feedback loop. The more you patrol a neighborhood, the more traffic violations you’ll find. Communities with no police force will never report any violations, while heavily patrolled communities will have the lion’s share of transgressions.\nThe model will use that data and make the problem worse: it will predict that new violations will happen in already problematic areas, sending more police to those communities at the expense of areas with lower reports. A few rounds of this, and you’ll have most reports from a few places while violations are rampant everywhere.\nRecommended reading\n\nCheck the description of a “Positive Feedback Loop” in Wikipedia.\n“How Positive Feedback Loops Are Hurting AI Applications” is an excellent article explaining the dangers of positive feedback loops in machine learning."
  },
  {
    "objectID": "20220909.html",
    "href": "20220909.html",
    "title": "Universal approximators",
    "section": "",
    "text": "She called them “universal approximators.” To classify, they had to approximate any measurable or continuous function up to any desired accuracy.\nWhich of the following would you consider universal approximators?\n\nNeural Networks\nSupport Vector Machines using a Gaussian kernel\nLinear Regression\nBoosted Decision Trees\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 2, 4\nExcept for linear regression, all the other three choices are universal approximators.\nThanks to the Universal approximation theorem, neural networks are a well-known member of this category. The theorem states that, when using non-linear activation functions, we can turn a two-layer neural network into a universal function approximator.\nThe authors of “A Note on the Universal Approximation Capability of Support Vector Machines” show that support vector machines with standard kernels can also approximate any measurable or continuous function.\nFinally, the “Universal approximation” chapter of Machine Learning Refined covers trees as one of the standard types of universal approximations.\nRecommended reading\n\n“A Note on the Universal Approximation Capability of Support Vector Machines” is a paper showing that SVMs with standard kernels can approximate any measurable or continuous function.\nChapter 12.5 “Universal approximation” of Machine Learning Refined covers kernels, neural networks, and trees as universal approximations."
  },
  {
    "objectID": "20221003.html",
    "href": "20221003.html",
    "title": "Date of sale",
    "section": "",
    "text": "She found a team working on a machine learning model for about a year with mediocre results. After a couple of weeks, Kylie proposed to do some feature engineering around a feature representing the sale date to help the model improve its predictions.\nWhich of the following are examples of feature engineering techniques that Kylie could do to improve the model?\n\nReplacing the feature representing the date of the sale with three separate columns for the year, month, and day.\nReplacing the feature representing the date of the sale with a single value containing the number of seconds since the year started.\nReplacing the feature representing the date of the sale with a single value that contains the number of the month.\nKeeping the date feature untouched and adding a new column representing the number of samples sold during the same month.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 2, 3, 4\nWe don’t know which of these options will be the most useful, but every one of them is an example of feature engineering that could help the model make better predictions.\nThe first three options are examples of how we can derive new features from a column in our dataset. In this case, Kylie can turn a date field into three components or simplify it by replacing it with a single value that keeps the necessary information for the model. Notice that, in these three cases, Kylie is not introducing anything new. Instead, she is transforming the data so the model can use it.\nThe fourth option is an example of frequency encoding, where Kylie counts how many products have been sold every month and creates a meta-feature with this information.\nRecommended reading\n\nThe Kaggle Book explains different feature engineering techniques.\nFeature Engineering for Machine Learning is an excellent book covering feature engineering."
  },
  {
    "objectID": "20220908.html",
    "href": "20220908.html",
    "title": "Leave one out",
    "section": "",
    "text": "She started the project by setting aside a portion of her data and built a model that, so far, impressed everyone with a strong performance. But the bubble burst when she trained and tested the model with a different portion of her dataset.\nRosie’s team recommended k-Fold cross-validation, but Rosie wanted to take the process to the extreme: she decided to create as many folds as samples in the dataset.\nWhich of the following statements correctly describes what Rosie should expect to happen:\n\nRosie’s method will be computationally cheaper than using fewer folds and result in a more reliable estimate of model performance.\nRosie’s method will be computationally more expensive than using fewer folds but will result in a more reliable estimate of model performance.\nRosie’s method will be computationally cheaper than using fewer folds but result in a less reliable estimate of model performance.\nRosie’s method will be computationally more expensive than using fewer folds and result in a less reliable estimate of model performance.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nRosie’s approach has a name: Leave-one-out cross-validation, a case of cross-validation where the number of folds equals the number of samples in the dataset.\nTo use leave-one-out cross-validation, we build one model for each sample in the dataset. We train each model using all data except one instance we later use to evaluate its performance. Finally, we compute the overall performance by averaging the result of each model.\nAssuming Rosie uses leave-one-out cross-validation on a dataset with 10,000 samples, she will need to train 10,000 models. Compare this with 10-Fold cross-validation, where she will only need to build ten models. Leave-one-out cross-validation is significantly more expensive than a process using fewer folds.\nOn the other hand, leave-one-out cross-validation will give Rosie a more robust estimate of model performance. Each sample has an opportunity to represent the entire dataset and contribute to the final evaluation, and this will result in a reliable and unbiased estimate of model performance.\nRecommended reading\n\n“LOOCV for Evaluating Machine Learning Algorithms” is an excellent introduction to leave-one-out cross-validation.\nCheck “A Quick Intro to Leave-One-Out Cross-Validation (LOOCV)” for a brief introduction to leave-one-out cross-validation."
  },
  {
    "objectID": "20220907.html",
    "href": "20220907.html",
    "title": "Normalizing inputs to a layer",
    "section": "",
    "text": "Liliana understood the idea of data normalization, but this new approach seemed bold and different.\nFortunately, she found plenty of documentation about this technique, called Batch Normalization. During the training process, Batch Normalization normalizes the input to each layer within the network.\nLiliana wants to write some notes before calling it a day. Which of the following are correct statements about Batch Normalization?\n\nBatch Normalization rescales the data to a mean of zero and a standard deviation of one.\nBatch Normalization rescales the data between zero and one.\nBatch Normalization makes the network less sensitive to the choice of weight initialization.\nBatch Normalization requires extra computations, slightly increasing the network’s total training time.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 3\nBatch Normalization is a popular technique used to train deep neural networks. It normalizes the input to a layer during every training iteration using a mini-batch of data. It smooths and simplifies the optimization function leading to a more stable and faster training.\nBatch Normalization works by scaling its input—the previous layer’s output—to a mean of zero and a standard deviation of one per mini-batch. Thus the first choice is correct and the second incorrect.\nAlthough correctly initializing a network can significantly impact convergence, the stability offered by Batch Normalization makes training deep neural networks less sensitive to a specific weight initialization scheme. Since Batch Normalization normalizes values, it reduces the likelihood of running into vanishing or exploding gradients.\nFinally, Batch Normalization does require extra computations, making individual iterations slower. However, it will dramatically reduce the number of iterations needed to achieve convergence, making the training process much faster. Therefore, the fourth choice is incorrect.\nRecommended reading\n\n“A Gentle Introduction to Batch Normalization for Deep Neural Networks” is a good introduction to Batch Normalization.\nAnother great article to understand the impact of Batch Normalization is “Why is Batch Normalization useful in Deep Neural Networks?”"
  },
  {
    "objectID": "20220901.html",
    "href": "20220901.html",
    "title": "Bounding boxes",
    "section": "",
    "text": "She wants to measure the performance of her object detection method, but there is a problem: The bounding boxes predicted by her model never match precisely the bounding boxes in the ground truth data.\nMagdalena needs a metric that tells her how well the two bounding boxes overlap. Based on this metric, she can define a threshold and decide if the predicted bounding box is correct or not.\nWhich metric can Magdalena use?\n\nThe percentage of the ground truth bounding box that’s covered by the predicted bounding box.\nThe percentage of the predicted bounding box that’s covered by the ground truth bounding box.\nThe intersection area of the predicted and ground truth bounding boxes divided by their union.\nThe intersection area of the predicted and ground truth bounding boxes multiplied by their union.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n3\nAfter a quick look at the choices, the first two options sound like plausible solutions. However, they have fundamental flaws that may allow a bad model to score high on these metrics.\nLet’s take the percentage of the ground-truth bounding box covered by the prediction. If our model learns to predict huge bounding boxes covering almost the whole image, we will always get a score close to 100%.\nIf we take the percentage of the predicted bounding box covered by the ground truth, we have the opposite problem: a small bounding box somewhere in the bounds of the ground truth will give us a 100% score. Therefore the first two choices are incorrect.\nAn excellent way to deal with these problematic cases is to take the so-called intersection over union measure (IoU for short, also called the Jaccard index). We compute the intersection area of the bounding boxes and divide it by their union. The union will increase if the predicted bounding box is too big, driving the score down. The small intersection area will lower the score if the bounding box is too small.\nFinally, the last choice is also incorrect. Multiplying the intersection by the union will not give us any relevant or helpful results.\nRecommended reading\n\nCheck “Intersection over Union (IoU) for object detection” for an entire explanation of this metric."
  },
  {
    "objectID": "20220825.html",
    "href": "20220825.html",
    "title": "Hailey’s labeling approach",
    "section": "",
    "text": "Despite having access to a massive dataset of x-ray pictures, she didn’t have too many labels. The company couldn’t afford to have doctors spending their time labeling samples, so she was in a catch-22 situation: to save the doctor’s time, she wanted to build a supervised learning model, but she couldn’t do that without having the doctors spend more time labeling data.\nHailey found a way to solve the problem: she built a model using the labels she had. Then started processing the unlabeled samples and setting aside those for which her model wasn’t confident.\nAfter each round, Hailey asked doctors only to spend time labeling those low-confidence samples, after which she built a new model and repeated the process.\nThis approach saved the company a massive amount of money.\nHow would you classify Hailey’s approach?\n\nHailey’s approach is a Weak Supervision technique.\nHailey’s approach is a Reinforcement Learning technique.\nHailey’s approach is an Unsupervised Learning technique.\nHailey’s approach is a Semi-supervised Learning technique.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n4\nHailey encountered a widespread problem we often face when we want to build machine learning models for companies. Sometimes, we have access to plenty of data, but labeling the dataset is time-consuming or hard to collect without a prohibitive cost.\nHailey did something clever: she only asked doctors to spend time labeling images that had the most value to her model. She built a good model with a fraction of the labels that otherwise would have been necessary.\nWe call this technique “Active Learning”, a semi-supervised learning technique. Hailey followed a pool-based sampling approach, where she queried the pool of unlabeled samples and used the confidence of her model to decide which instances to label next.\nRecommended reading\n\n“Active Learning” is a short introduction to active learning and how the process works.\nIf you are serious about Active Learning, “Active Learning Literature Survey” is the publication you want to read."
  },
  {
    "objectID": "20220823.html",
    "href": "20220823.html",
    "title": "Bloody snake",
    "section": "",
    "text": "Don’t ask how you got here. You don’t have time for these questions. You need to focus on staying alive.\nA massive snake with three heads and bloody eyes comes charging after you. It’s not looking good!\nYou have enough breath to read a clue written on the walls at the last second: “bootstrap it, and you’ll live,” it says.\nWhere would you hide? Only one of these places will save your life:\n\nBehind a tree\nIn the forest\nIn a cave\nIn a hole\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nI hope this was a fun question.\nAs you would expect, the answer is in the clue written on the walls. “Bootstrap it, and you’ll live” doesn’t say much until you see the four hiding spots: A tree, a forest, a cave, and a hole.\nBootstrap aggregating, also called “bagging,” is a popular machine learning ensembling technique. We usually use bagging with decision trees, and one of the hiding spots is a tree, so that could be the answer! In reality, there’s an even better answer.\nRandom Forest is an algorithm that consists of many individual decision trees. It uses bootstrap aggregating to combine these trees to reach a solution much better than the one provided by any of the individual trees.\n“Bootstrapping” is more closely related to Random Forest than Decision Trees, so you’ll stay alive if you hide in the forest.\nRecommended reading\n\n“Bagging and Random Forest Ensemble Algorithms for Machine Learning” is a great introduction to bagging and Random Forest.\nCheck out “Understanding Random Forest” to understand how it works and why it’s effective."
  },
  {
    "objectID": "20220919.html",
    "href": "20220919.html",
    "title": "Learnable parameters",
    "section": "",
    "text": "Here is the core of the code she put together:\nmodel = keras.Sequential([\n    keras.Input(shape=(28, 28, 1)),\n    layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n    layers.MaxPooling2D(pool_size=(2, 2)),\n    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n    layers.MaxPooling2D(pool_size=(2, 2)),\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation=\"softmax\"),\n])\nBased on the above code fragment, what are the correct statements regarding each layer’s parameters (weights and biases)?\n\nThe first convolutional layer has a total of 21,632 parameters.\nThe first max pooling layer has a total of 5,408 parameters.\nThe second convolutional layer has a total of 18,496 parameters.\nThe fully-connected layer has a total of 16,010 parameters.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n3, 4\nWe can compute the number of parameters of a convolutional layer using the following formula:\nparameters = k * (f * h * w + 1)\nWhere k corresponds to the number of output filters from this layer, f corresponds to the number of filters coming from the previous layer, h corresponds to the kernel height and w to the kernel width. The value 1 corresponds to the bias parameter related to each filter. Here is the complete calculation for the first convolutional layer:\nparameters = k * (f * h * w + 1)\nparameters = 32 * (1 * 3 * 3 + 1) = 320\nMax pooling layers don’t have any parameters because they don’t learn anything. The input to the first max pooling layer is 26x26x32, but the layer doesn’t have any weights or biases associated with it.\nThe second convolutional layer does have 18,496 parameters. Let’s check:\nparameters = k * (f * h * w + 1)\nparameters = 64 * (32 * 3 * 3 + 1) = 18,496\nFinally, the fully-connected layer has 16,010 parameters. To compute this, we need to calculate the size of each layer to understand the input to the fully-connected layer:\nInput: 28x28x1 = 784\nConv2D: 26x26x32 = 21,632\nMaxPool2D: 13x13x32 = 5,408\nConv2D: 11x11x64 = 7,744\nMaxPool2D: 5x5x64 = 1,600\nWe can use the same formula to compute the number of parameters of the output layer:\nparameters = 10 * (1600 + 1) = 16010\nRecommended reading\n\nCheck “Understanding and Calculating the number of Parameters in Convolution Neural Networks (CNNs)” for instructions on how to compute the number of learnable parameters.\n“Simple MNIST convnet” is a Keras example showing this particular code fragment."
  },
  {
    "objectID": "20220831.html",
    "href": "20220831.html",
    "title": "Vintage shoes",
    "section": "",
    "text": "Business is going great, and one of her main issues is determining the price of new inventory as it comes in. Sam would like to create a machine learning model that automatically selects the best price using the historical performance.\nSam was a data scientist in a previous life, so she was surprised when she found some documentation recommending a new loss function: The Root Mean Squared Log Error (RMSLE.) This function seemed better than Root Mean Squared Error (RMSE) for her use case.\nWhich of the following are some of the differences between RMSLE and RMSE?\n\nRMSLE penalizes coming under the actual value much more than coming above the actual value. On the other hand, RMSE penalizes both cases in the same way.\nRMSLE is not sensitive to outliers as RMSE is. Whenever we have outliers, the result of RMSE can explode while RMSLE will scale down the outliers.\nRMSLE measures the error by only focusing on the predicted value. On the other hand, RMSE uses the difference between the predicted and the actual value to compute the error.\nRMSLE focuses on the relative error between two values. When using RMSLE, the scale of the error is not significant. On the other hand, the result of RMSE increases in magnitude if the error scale increases.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 2, 4\nCompared to RMSE, RMSLE is a relatively new metric. Here is the formula:\n\\[\n\\sqrt{ \\frac{1}{n} \\sum_{i=1}^n (\\log(x_i + 1) - \\log(y_i + 1))^2 }\n\\]\nIf you compare this formula with the RMSE formula, you’ll notice they are almost the same, with the difference of RMSLE computing the log of both the predicted and actual values. This small difference comes with some essential properties.\nFirst, RMSLE penalizes underestimates much more than overestimates. To understand the reason, take a look at the following charts that I took from “What’s the Difference Between RMSE and RMSLE?”:\n\nNotice how RMSLE penalizes negative values much more heavily than positive ones, while RMSE equally penalizes both cases. Therefore, the first choice is correct.\nThe second choice is also correct. The log that RMSLE uses squashes outliers, while RMSE amplifies their effect. If you want your model to remain unaffected by outliers, RMSLE is a good candidate.\nThe third choice is incorrect because both RMSE and RMSLE use the predicted and actual values. Predicted values alone don’t tell us anything; comparing them with the actual value is critical.\nFinally, the fourth choice is also correct. For example, imagine a case where the model predicts 30 when the actual value was 40 and another case where the prediction was 300 when the actual value was 400. The RMSE of the second case is ten times more than the RMSE of the first case. However, the RMSLE will score both cases the same. The RMSLE measures the relative error, while the RMSE measures the absolute error.\nIn summary, the first, second, and fourth choices are correct.\nRecommended reading\n\n“What’s the Difference Between RMSE and RMSLE?” covers really well every one of the differences between these two functions.\nAnother article that covers the differences between some of the most popular functions is “Evaluation Metrics for Regression models”"
  },
  {
    "objectID": "20220914.html",
    "href": "20220914.html",
    "title": "The final exam",
    "section": "",
    "text": "In a last attempt to do something original, she copied an example code fragment from the Keras website and decided to ask a few questions about it:\nmodel = keras.Sequential([\n    keras.Input(shape=(28, 28, 1)),\n    layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n    layers.MaxPooling2D(pool_size=(2, 2)),\n    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n    layers.MaxPooling2D(pool_size=(2, 2)),\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation=\"softmax\"),\n])\nBased on the code above, which of the questions Emersyn wrote are correct?\n\nThe kernel and pool size should have the same value. The model will likely fail to learn anything meaningful.\nThe default activation function in MaxPooling2D layers is ReLU, which is why the code doesn’t explicitly use it.\nThe Dropout right before the output layer will cut down the number of learnable parameters from 1,600 to 800.\nThe softmax activation function in the last layer hints that this is a multi-class classification model.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n4\nThe kernel size in convolutional layers and the pool size of the pooling layer don’t need to have the same size. The code fragment is indeed part of a working example, and it’s correctly configured.\nMaxPooling2D layers don’t use activation functions. That’s the reason the code doesn’t specify one. A max pooling operation calculates the maximum value in each patch of each feature map. Keras will throw an error if you try to set an activation function on the MaxPooling2D layer.\nThe Dropout right before the output layer doesn’t reduce the number of learnable parameters in half. Instead, the Dropout will set the value of 50% of the neurons to 0.\nFinally, the softmax activation function and the size of the output layer point to a multi-class classification problem. Remember that softmax converts a vector of numbers into a vector of probabilities, which we need in multi-class classification tasks.\nRecommended reading\n\nFor more information about convolutional layers, check “How Do Convolutional Layers Work in Deep Learning Neural Networks?”.\nCheck “A Gentle Introduction to Pooling Layers for Convolutional Neural Networks” for more information about how max pooling works.\n“A Gentle Introduction to Dropout for Regularizing Deep Neural Networks” is an excellent introduction to Dropout.\n“Simple MNIST convnet” is a Keras example showing this particular code fragment."
  },
  {
    "objectID": "20221001.html",
    "href": "20221001.html",
    "title": "High training",
    "section": "",
    "text": "As you can see, after finishing training, the loss stays too high.\nWhat’s a reasonable conclusion about this machine learning model?\n\nThe model is overfitting.\nThe model is underfitting.\nThe model is neither overfitting nor underfitting.\nThe model is either overfitting or underfitting, but we can’t say for sure.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nA good model should capture valuable patterns in the data and discard any noise that doesn’t help with predictions. An overfitting model will fit that noise. An underfitting model will not capture the relevant patterns in the dataset.\nAn overfitting model should not have any problems with the training data, so we should expect a low training loss. An underfitting model should struggle with the training data, so its training loss will be high.\nThis model shows a high training loss, which we expect for an underfitting model.\nRecommended reading\n\nCheck “Overfitting and Underfitting with Learning Curves” for an introduction to two fundamental concepts in machine learning through the lens of learning curves."
  },
  {
    "objectID": "20220926.html",
    "href": "20220926.html",
    "title": "Defective chips",
    "section": "",
    "text": "They trained their autoencoder on millions of pictures of working chips to generate a compressed representation of the common characteristics of a chip in good condition.\nThey run every new picture in production through the autoencoder to determine whether there’s a problem with that particular chip.\nWhich of the following is the company’s process to determine whether the chip is in good working condition?\n\nThe autoencoder’s output is a softmax vector that classifies the picture into two classes. If the largest value in the vector corresponds to the anomaly class, the company knows this is a defective chip.\nAutoencoders work like a binary classification model, where any output value greater than a predefined threshold indicates the input is an anomaly.\nThe company computes the error between the model’s original input and output. The picture represents a defective chip if the error exceeds a predefined threshold.\nThe autoencoder’s output is a value indicating how different the input image is from a working chip, so the largest this value is, the more likely it’s for the input to be a defective chip.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n3\nAutoencoders are learning algorithms that can help with anomaly detection problems.\nAn autoencoder is a neural network that we can split into three sections: an encoder, a bottleneck, and a decoder. The encoder compresses the original input into an intermediate representation, and the decoder reverses the process to reconstruct the original data. The bottleneck sits between the encoder and decoder and is the section that stores the compressed representation of the data.\nIn this particular problem, we can train an autoencoder by showing it images of working chips and expecting the model to reproduce the same input image. In other words, the autoencoder’s input and expected output are the same. If we compute the similarity between the input and output images, we can determine whether the original picture belongs to a working or defective chip.\nA defective chip will have specific visual characteristics that will be impossible for the autoencoder to reproduce. Remember that the autoencoder learned on a dataset of working samples, so it won’t have any notion of features that aren’t common among chips in good status.\nWe should expect a reproduction error larger than normal whenever we input a defective chip into the autoencoder. Therefore, the third option is the correct answer to this question.\nRecommended reading\n\nCheck “Autoencoders” for an introduction to a learning technique to represent data efficiently using neural networks."
  },
  {
    "objectID": "20221005.html",
    "href": "20221005.html",
    "title": "Tossing a fair coin",
    "section": "",
    "text": "I’ll give the coin to you, and I want you to flip it five times.\nLet’s image two different scenarios: one where you get the coin to land five consecutive times on heads, and the second where the coin lands four straight times on heads, then on tails.\nWhich of those two scenarios is most likely to happen?\n\nGetting five straight heads is more likely than getting four heads followed by tails.\nGetting four heads followed by tails is more likely than getting five consecutive heads.\nGetting four heads followed by tails is equally likely than getting five successive heads.\nIf the coin is fair, this is a random event, and we can’t say which one is more likely than the other.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n3\nBoth scenarios are equally likely to happen with a probability of 1/32.\nThis scenario is an example of the Gambler’s Fallacy, the irrational belief that prior outcomes in a series of events affect the probability of a future outcome, even though the events are independent and identically distributed.\nHere is a quote from the Gambler’s Fallacy Wikipedia page:\n\nIf after tossing four heads in a row, the next coin toss also came up heads, it would complete a run of five successive heads. Since the probability of a run of five successive heads is 1/32, a person might believe that the next flip would be more likely to come up tails rather than heads again. This is incorrect and is an example of the gambler’s fallacy. The event “5 heads in a row” and the event “first 4 heads, then a tails” are equally likely, each having probability 1/32.\n\nRecommended reading\n\nCheck “Gambler’s Fallacy” on Wikipedia for an introduction to the situation described in this question."
  },
  {
    "objectID": "20221011.html",
    "href": "20221011.html",
    "title": "Extra regularization",
    "section": "",
    "text": "But everything comes at a price. Batch Normalization introduces some regularization to the network, and Eloise knows this will affect the Dropout she is currently using.\nHow should Eloise adjust the Dropout her network is using?\n\nAfter adding Batch Normalization, Eloise should remove the Dropout she is currently using.\nAfter adding Batch Normalization, Eloise should slightly decrease the amount of Dropout she is currently using.\nAfter adding Batch Normalization, Eloise should slightly increase the amount of Dropout she is currently using.\nAfter adding Batch Normalization, Eloise should significantly increase the amount of Dropout she is using.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nBatch Normalization is a popular technique used to train deep neural networks. It normalizes the input to a layer during every training iteration using a mini-batch of data. It smooths and simplifies the optimization function leading to a more stable and faster training.\nBatch Normalization acts as a regularizer. The mean of a mini-batch of data is a noisier version of the true mean of the population, and when used to normalize the data, it adds randomness to the optimization process. Eloise will need to adjust the Dropout she is using to accommodate this extra regularization.\nKeep in mind that Batch Normalization doesn’t introduce a lot of regularization, so it’s unlikely that Eloise can remove all of the Dropout she is using. She should, however, slightly decrease the Dropout she is using to account for the extra regularization.\nRecommended reading\n\n“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift” is the original paper introducing Batch Normalization.\n“Intro to Optimization in Deep Learning: Busting the Myth About Batch Normalization” is an excellent blog post challenging some of the ideas from the original paper.\n“A Gentle Introduction to Batch Normalization for Deep Neural Networks” is a good introduction to Batch Normalization."
  },
  {
    "objectID": "20220826.html",
    "href": "20220826.html",
    "title": "The weird puzzle room",
    "section": "",
    "text": "But this room wasn’t like every other room.\nThey found the final puzzle behind a painting hanging from the wall. Four numbered little doors. The key hid behind one of them and a losing score behind the other three.\nAbove the doors, they found the question: “What’s the optimal depth of a decision tree?”\nWhat door will let Vivian’s family out of the room?\n\nThe optimal value for a decision tree’s depth is setting it to the number of training samples minus one.\nThe optimal value for a decision tree’s depth is setting it to the logarithm of the number of training samples.\nThe optimal value for a decision tree’s depth is setting it as low as we possibly can.\nThe optimal value for a decision tree’s depth is setting it as high as we possibly can.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n3\nWe need to revisit a few concepts and establish constraints to answer this question. How do we know a decision tree has an optimal depth?\nFirst, the depth of a decision tree is the length of the longest path from a root to a leaf. There’s a tradeoff to keep in mind related to this value. Deeper trees will lead to more complex models prone to overfitting. Shallower trees will have the opposite problem: less complex models prone to underfitting. Finding the appropriate depth is critical to getting good results.\nTo determine the optimal depth, we then need to establish how we will measure the performance of the decision tree. Since the goal is to build a model that produces good predictions, I’ll assume that our goal is to find the optimal depth to get the best predictions possible on a test dataset.\nLet’s start with the first choice that argues that we should set the depth at the number of training samples minus one. This is the maximum theoretical depth of a decision tree, but it’s not the optimal value. If we make the tree this deep, we will undoubtedly overfit the training data and perform poorly on any unseen data.\nThe second choice is also incorrect. Although this option is probably better than setting the depth to the number of samples, it will still lead to overfitting. For example, for a dataset with 1024 training samples, we will need a decision tree with 10 levels and a tree of depth 10, which requires a total of 2^10 = 1024 nodes. This choice will lead to a tree with as many nodes as samples, which will cause the tree to overfit.\nWe can analyze the third and fourth choices together. Are we better off setting the depth of a decision tree as low or as high as possible? Let’s pick two hypothetical decision trees, each producing the same results, but one deeper than the other. Which of these two models would you use?\nLess complex classifiers will usually generalize better than more complex ones. We should remove any non-critical or redundant sections of a decision tree, which generally leads to a much better model. Decision tree pruning is one technique used to accomplish this.\nIf you have two decision trees with the same predictive power in your test dataset, always pick the simpler one. Therefore, we should always set the depth as low as possible without affecting the model’s predictive capabilities.\nIn summary, the third choice is the correct answer to this question.\nRecommended reading\n\n“How to tune a Decision Tree?” is a great article talking about different ways to tune a decision tree, including the effects of setting the maximum depth hyperparameter.\n“Decision tree pruning” is a Wikipedia article covering pruning and its effects.\nA great article to understand the relationship between the bias-variance tradeoff, overfitting, and under-fitting is “How To Find Decision Tree Depth via Cross-Validation”."
  },
  {
    "objectID": "20220922.html",
    "href": "20220922.html",
    "title": "Bagging or Boosting?",
    "section": "",
    "text": "There’s only one question for her to answer: Should she use bagging or boosting?\nBoth techniques have different advantages and disadvantages, and Katherina wants to ensure she evaluates them correctly before committing to one solution.\nWhich of the following statements are true about bagging and boosting?\n\nBagging trains individual models sequentially, using the results from the previous model to inform the selection of training samples.\nBoosting trains individual models sequentially, using the results from the previous model to inform the selection of training samples.\nBagging trains a group of models, each using a subset of data selected randomly with replacement from the original dataset.\nEach model receives equal weight in bagging to compute the final prediction while boosting uses some way of weighing each model.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2, 3, 4\nEnsembling is where we combine a group of models to produce a new model that yields better results than any initial individual models. Bagging and boosting are two popular ensemble techniques.\nBagging trains a group of models in parallel and independently from each other. Each model uses a subset of the data randomly selected with replacement from the original dataset. In contrast, Boosting trains a group of learners sequentially, using the results from each model to inform which samples to use to train the next model.\nThis summary helps us conclude that the first choice is incorrect, but the second and third choices are correct.\nFinally, when computing the final prediction, bagging averages out the results of each model. Boosting, however, weights each model depending on its performance. Therefore, the fourth choice is also correct.\nRecommended reading\n\nCheck “Bagging vs. Boosting in Machine Learning: Difference Between Bagging and Boosting for a detailed comparison between both techniques.\nWhat is the difference between Bagging and Boosting? is another great summary of both techniques and their advantages and disadvantages."
  },
  {
    "objectID": "20221008.html",
    "href": "20221008.html",
    "title": "False Negatives",
    "section": "",
    "text": "Assume that class A represents the outcomes of the model we are interested in finding.\nWhat’s the total of False Negatives on this evaluation round?\n\nFalse Negatives are class A samples the model predicted as class B, so the answer is 7.\nFalse Negatives are class B samples the model predicted as class A, so the answer is 13.\nFalse Negatives are class A samples the model predicted as class A, so the answer is 52.\nFalse Negatives are class B samples the model predicted as class B, so the answer is 28.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1\nWe are interested in samples from class A which means we will treat class A as our “Positive” samples and class B as our “Negative” samples.\nIf we replace the classes in the confusion matrix with Positive and Negative instead of “A” and “B,” it’s much easier to reason about the model’s number of False Negatives:\n\nFalse Negatives are those samples that we expect to be Positive (class A), but the model predicted as Negative (class B.) Therefore, the correct answer to the question is 7. You can see every combination in the image above.\nRecommended reading\n\nCheck “Confusion Matrix” for a full explanation of how a confusion matrix works and how you can use them as part of your work.\nCheck “When accuracy doesn’t help” for an introduction to precision, recall, and f1-score metrics to measure a machine learning model’s performance."
  },
  {
    "objectID": "20220902.html",
    "href": "20220902.html",
    "title": "Stock prices",
    "section": "",
    "text": "A fund manager wants George’s team to design a machine learning model that evaluates the price of stocks. The model should use publicly available information to predict the true share price of each company.\nThat’s all the fund manager needs. He will take it from there and look deeper into under-priced stocks (where the true price is higher than the current market price.)\nThe model will act as a pre-filter to sort out potential companies, and the fund manager will do a more detailed analysis before deciding whether to invest in the stock. The fund manager doesn’t care if there are occasional wrong predictions, even by a large margin, but he doesn’t want to have too many false positives because detailed research will cost a lot of time.\nGeorge is trying to decide which loss function the team should use for the model.\nWhich of the following loss functions do you think are suitable for this problem?\n\nMean Squared Error (MSE)\nMean Absolute Error (MAE)\nBinary Cross-entropy Loss\nHuber Loss\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2, 4\nWe don’t have too much information about the problem, but the client has clear priorities regarding the errors they are willing to tolerate. Significant outliers are not a big problem because the fund manager will spot them quickly and reject them. However, they expect excellent overall performance to avoid wasting time researching unpromising stocks.\nWith this in mind, the Mean Absolute Error (MAE) and the Huber loss seem like better options than the Mean Squared Error (MSE.) The MSE penalizes high errors of the model much more than the others. This means that the model will try to learn to avoid big mistakes in the prediction, which will likely come at the cost of worse overall performance. We don’t want this.\nOn the other hand, the MAE and the Huber loss are more robust to outliers, and they will not dominate the training process. Therefore, the second and fourth choices are the correct answers.\nThe cross-entropy loss doesn’t apply to regression problems, so the third choice is also incorrect.\nRecommended reading\n\nCheck “Regression Metrics for Machine Learning” for an overview of some of the most popular metrics used for regression problems.\n“RMSE vs MAE, which should I use?” is a great summary by Stephen Allwright about the properties of these two functions and how you should think about them. For more information about the Huber loss, take a look at “Huber Loss: Why Is It, Like How It Is?”."
  },
  {
    "objectID": "20220920.html",
    "href": "20220920.html",
    "title": "Augmenting the dataset",
    "section": "",
    "text": "She was building a deep network to classify pictures. From the beginning, her Achilles’ heel has been the size of her dataset. One of her teammates recommended she use a few data augmentation techniques.\nEsther was all-in. Although she wasn’t sure about the advantages of data augmentation, she was willing to do some research and start using it.\nWhich of the following statements about data augmentation are true?\n\nEsther can use data augmentation to expand her training dataset and assist her model in extracting and learning features regardless of their position, size, rotation, etc.\nEsther can use data augmentation to expand the test dataset, have the model predict the original image plus each copy, and return an ensemble of those predictions.\nEsther will benefit from the ability of data augmentation to act as a regularizer and help reduce overfitting.\nEsther has to be careful because data augmentation will reduce the ability of her model to generalize to unseen images.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n1, 2, 3\nOne significant advantage of data augmentation is its ability to make a model resilient to variations in the data. For example, assuming we are working with images, we can use data augmentation to generate synthetic copies of each picture and help the model learn features regardless of where and how they appear.\nA few popular augmentation techniques when working with images are small rotations, horizontal and vertical flipping, turning the picture to grayscale, or cropping the image at different scales. The following example shows four versions of an image generated by changing the original picture’s brightness, contrast, saturation, and hue:\n\nData augmentation is also helpful during testing time: Test-Time Augmentation is a technique where we augment samples before running them through a model, then average the prediction results. Test-Time Augmentation often results in better predictive performance.\nInstead of predicting an individual sample from the test set, we can augment it and run each copy through the model. Esther is working on a classification problem, so her model will output a softmax vector for each sample. She can then average all these vectors and use the result to choose the correct class representing the original sample.\nUsing data augmentation, Esther can reduce overfitting and help her model perform better on unseen data. Data augmentation has a regularization effect. Increasing the training data through data augmentation decreases the model’s variance and, in turn, increases the model’s generalization ability. Therefore, the third choice is correct, but the fourth one is not.\nRecommended reading\n\n“The Essential Guide to Data Augmentation in Deep Learning” is an excellent article discussing data augmentation in detail.\nCheck “Test-Time augmentation” for an introduction that will help you make better predictions with your machine learning model."
  },
  {
    "objectID": "20220910.html",
    "href": "20220910.html",
    "title": "Missing answers",
    "section": "",
    "text": "It was clear to Amaya that not disclosing the answer to the question was as important as the answer itself, so she wanted to consider this when preparing the data.\nAmaya is ready to build a machine learning model but must first deal with the missing answers.\nWhich of the following should be the best strategy that Amaya should pursue?\n\nAmaya should remove the column that contains the missing values.\nAmaya should add a new column to the dataset to flag rows with missing values and then replace those values with a reasonable answer.\nAmaya should replace the missing values with a reasonable answer.\nAmaya should predict the missing values using a separate machine learning model.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n2\nAmaya knows she must do something with the missing answers before training a machine learning model, and imputing these values seem like a good solution. For example, she could replace the missing values with their mean, median, or mode.\nBut this approach has a problem: Amaya thinks having many participants not disclose the answer to the question is as important as the answer itself. If she replaces the missing values, she will lose that information.\nA great approach to avoid losing that information is to add a new binary column to the dataset. This column will flag every participant that didn’t answer the question. Assuming avoiding to answer is not a random event, this new column will help with the predictive performance of Amaya’s model.\nTherefore, the best strategy for Amaya is to add this new column right before imputing missing values.\nRecommended reading\n\n“Handling Missing Values” is a Kaggle notebook that goes over a few strategies to handle missing values including adding an extra column to flag these rows.\nCheck “Imputation of missing values” for some information about how Scikit-Learn handles missing values.\n“How to Handle Missing Data with Python” is an excellent post discussing different strategies in Python."
  },
  {
    "objectID": "20220828.html",
    "href": "20220828.html",
    "title": "The first exam",
    "section": "",
    "text": "Almost 50 students submitted a document explaining the neural network architecture they used to solve a problem and a complete analysis of their results.\nAyla opens the first paper and sees the following chart, showing the model’s training and validation loss over the number of iterations (epochs):\n\nWhich of the following is the correct feedback Ayla should give this student?\n\nThe training loss decreases continuously, indicating that the model is doing well.\nThe separation between the training and the validation curves indicates the training set is too complex for the model to learn.\nThe training and validation loss continue to decrease until the end of the training process, indicating the model is overfitting.\nThe training and validation loss continue to decrease until the end of the training process, indicating the model is underfitting.\n\n\n\n\n\n\n\nExpand to see the answer\n\n\n\n\n\n4\nAnalyzing learning curves is one of the fundamental skills you should build in your career. There are different learning curves, but we will focus on Ayla’s chart here.\nFirst, notice that the chart shows the loss—or error—as we increase the number of training iterations. A good mental model is to look at this the following way: “as we keep training, how much better the model gets?” Since we are displaying the loss, larger values are worse, so having both lines decrease is a good sign.\nWe have two lines in the chart: one representing the loss we get during training, the other representing the loss during the validation process. How these lines look concerning each other is essential. Most of the time, one of the lines alone wouldn’t give you a complete picture of the situation.\nLet’s start with the first choice that argues that a training loss that’s continually decreasing means that Ayla’s model is doing a good job. While this could be a good sign, it’s not a sufficient explanation to draw any conclusions about the quality of the model. For example, the model could be memorizing the dataset (overfitting), and while the training loss is decreasing, we will end up with a flawed model. Therefore, the first choice is incorrect.\nThe chart shows a gap between the training and the validation loss. Even when the student didn’t train the model for too long (about 50 epochs), the gap indicates the model can achieve a better loss in the validation dataset. This, however, doesn’t mean the training dataset is too complex for the model to learn. The chart shows how the training loss continually decreases until the end of the process, so the second choice is also incorrect.\nThe critical insight is that the training and validation losses decrease during the entire process. What would you say if you were to guess what would happen right after the 50th epoch? Looking at the chart, it seems clear that both the training and validation losses have the potential to keep decreasing, and therefore the student stopped the training process too early.\nWhat does this mean? Is the model overfitting or underfitting?\nThe model is not maximizing its potential. If we let it train for longer, the model will likely continue learning. This is a characteristic of underfitting models: they aren’t taking full advantage of their data. Therefore, the third choice is incorrect, and the fourth is the correct answer to this question.\nRecommended reading\n\n“How to use Learning Curves to Diagnose Machine Learning Model Performance” is the article that inspired this question and from where I borrowed Ayla’s chart.\nWikipedia’s introduction to Learning curves."
  }
]